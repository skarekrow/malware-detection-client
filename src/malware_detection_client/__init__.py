#!/usr/libexec/platform-python
import json
import os
import sys
import time
from datetime import datetime
import subprocess
import logging
from iniparse import RawConfigParser
from tempfile import NamedTemporaryFile
import re
try:
    # Python 2.7
    from urllib import quote as urlencode
    from urlparse import urlparse
except ImportError:
    # Python 3.6
    from urllib.parse import urlparse, quote as urlencode

from .process_include_exclude_items import process_items
from .utils import (
    create_session, get_insights_id, get_identity_header,
    create_new_conf_file, find_yara, run_cmd, string_to_bool,
    logger, CONF_FILE,
    get_scan_since_timestamp, find_modified_in_directory, find_modified_include_items
)

__version__ = "0.1.3"
MIN_YARA_VERSION = "4.1.0"


class MalwareDetectionClient:

    def __init__(self,
                 rules_location='',
                 results_url='',
                 scan_entity=None,
                 scan_recent=None,
                 saved_rules_file=None,
                 exclude_rules='',
                 conf_file=CONF_FILE,
                 new_conf_file=None,
                 test_option=False,
                 no_upload=False,
                 debug=None):

        self.matches = 0
        self.no_upload = no_upload
        self.test_option = test_option

        self.conf_file = conf_file
        if new_conf_file:
            create_new_conf_file(new_conf_file, rules_location, results_url)

        self.cfg = RawConfigParser()
        try:
            with open(self.conf_file):
                pass
            self.cfg.read(self.conf_file)
        except Exception:
            logger.error("Unable to read configuration file '%s'. "
                         "Ensure it exists and have permission to read it (eg you're running via sudo/as root)", self.conf_file)
            sys.exit(1)

        if debug:
            log_level = 'DEBUG'
        elif os.getenv('LOG_LEVEL'):
            log_level = os.getenv('LOG_LEVEL').upper()
        else:
            log_level = self._get_config_option('client', 'log_level', 'INFO').upper()
        logger.setLevel(logging.getLevelName(log_level))
        logging.getLogger('requests').setLevel(log_level)

        # Find the yara binary on the local system.  Exit if not found
        self.yara_binary = self._get_config_option('yara', 'location')
        if self.yara_binary and not os.path.isfile(self.yara_binary):
            logger.error("Couldn't find the specified yara binary '%s'.  Please check it exists", self.yara_binary)
            sys.exit(1)
        elif not self.yara_binary:
            self.yara_binary = find_yara()
        logger.debug("Using yara binary: %s", self.yara_binary)

        # Check the installed yara version >= MIN_YARA_VERSION
        installed_yara_version = run_cmd([self.yara_binary, '--version'])
        try:
            if float(installed_yara_version[:3]) < float(MIN_YARA_VERSION[:3]):
                logger.error("Found yara version %s, but malware-detection-client needs version >= %s"
                             % (installed_yara_version, MIN_YARA_VERSION))
                sys.exit(1)
        except (IndexError, ValueError):
            # Either the indexing or converting to float went wrong.  Keep going anyway
            pass

        # Get the system's insights id from /etc/insights-client/machine-id.  Exit if it can't be read
        self.insights_id = get_insights_id()

        # Get the location of the rules, ie a file or URL, or exit if not specified
        self.rules_location = rules_location if rules_location else self._get_config_option('client', 'rules_location', '')
        if not self.rules_location:
            logger.error("Please check that 'rules_location' has a value in the configuration file '%s'",
                         self.conf_file)
            sys.exit(1)

        # Check if the scan_recent option was specified, either via command line or the configuration file
        # If so, ensure the value is valid and get the timestamp for which file modification times must be newer than
        self.scan_since = {'timestamp': None, 'datetime': None}
        if scan_recent:
            self.scan_since['timestamp'] = get_scan_since_timestamp(scan_recent)
        elif self._get_config_option('client', 'scan_recent'):
            self.scan_since['timestamp'] = get_scan_since_timestamp(self._get_config_option('client', 'scan_recent'))
        if self.scan_since['timestamp']:
            self.scan_since['datetime'] = datetime.fromtimestamp(self.scan_since['timestamp']).strftime('%Y-%m-%d %H:%M:%S')
            logger.debug('Scanning for files created/modified since %s', self.scan_since['datetime'])

        # Get thing to scan from passed in args, if any
        # Unless test_option is also specified then ignore the passed in value of scan_entity
        self.scan_fsobjects = []
        self.scan_pids = []
        if scan_entity and not self.test_option:
            if os.path.exists(scan_entity):
                if re.match('^/[/.]*$', scan_entity):
                    logger.info("The root directory '/' was specified to be scanned. "
                                "All files on the system, less any excluded files, will be scanned.")
                    self.scan_fsobjects = sorted(list(map(lambda x: "/" + x, os.listdir('/'))))
                else:
                    self.scan_fsobjects = [os.path.abspath(scan_entity)]
                self.do_filesystem_scan = True
                self.do_process_scan = False
                logger.info("Scan argument: %s %s", "directory" if os.path.isdir(scan_entity) else "file", scan_entity)
            elif os.path.exists('/proc/' + scan_entity):
                self.scan_pids = [int(scan_entity)]
                self.do_filesystem_scan = False
                self.do_process_scan = True
                logger.info("Scan argument: process %s", scan_entity)
                if scan_recent:
                    logger.info("Found the --scan PID option and --scan-recent option used together.  "
                                "Ignoring the --scan-recent option which only applies to files")
                    self.scan_since['timestamp'] = None
            else:
                logger.error("Couldn't find the specified file/directory/process id to scan: %s.  Exiting", scan_entity)
                sys.exit(1)

        if self.test_option:
            if scan_entity:
                logger.info("Test option and scan option used together.  Ignoring scan option value '%s'", scan_entity)
            if scan_recent:
                logger.info("Found the --test option and --scan-recent options used together.  "
                            "Ignoring the --scan-recent option")
                self.scan_since['timestamp'] = None

            # Get the test rule (test_rule.yar) from rules_location instead of the specified rule (ie signature.yar)
            self.rules_location = self._get_test_rule_location(self.rules_location)

            # Try to find the malware client on the filesystem and the running client processes
            # If the rpm is installed, look for the malware client's main __init__.py file in /usr/lib/python
            # Otherwise, do a scan of /usr/lib anyway, but __init__.py probably won't be there
            output = list(filter(lambda x: x.endswith('/__init__.py'),
                                 run_cmd(["/bin/rpm", "-ql", "malware-detection-client"]).splitlines()))
            output = os.path.dirname(output[0]) if output else ''
            self.scan_fsobjects = [output] if output.startswith('/usr/lib/python') else ['/usr/lib']
            self.scan_pids = [str(os.getpid())]

            self.do_filesystem_scan = True
            self.do_process_scan = True
            logger.info("Test scanning directory: '%s', and pid: '%s'", self.scan_fsobjects[0], self.scan_pids[0])

        logger.info("Getting the rules from '%s'", self.rules_location)

        # Get the location where the scan results are to be uploaded, or exit if not specified
        self.results_url = results_url if results_url else self._get_config_option('client', 'results_location', '')
        if self.no_upload:
            logger.info("Scan results will not be uploaded because of --no-upload option")
        else:
            if not self.results_url:
                logger.error("Please check that 'results_location' has a value in the configuration file '%s'",
                             self.conf_file)
                sys.exit(1)
            logger.debug("Scan results will be uploaded to %s", self.results_url)

        # If we are either downloading rules or uploading results via http(s) then create a requests session
        # - set the http proxy if defined
        # - set the cert/key auth options if using console.redhat.com
        # - set the auth cookie (JWT token) if using keycloak for auth
        if self.rules_location.startswith('http') or (self.results_url.startswith('http') and not self.no_upload):
            self.session = create_session()
            proxy = self._get_config_option('client', 'proxy_url')
            self.session.proxies.update({'http': proxy, 'https': proxy} if proxy else {})
            self.session.headers.update(get_identity_header(self._get_config_option('client', 'account_number')))

            # Do we need to use authentication with the backend?
            if any([url for url in [self.rules_location, self.results_url] if 'console.redhat.com' in url]):
                self._get_cert_auth()
            elif self._get_config_option('auth', 'keycloak_url'):
                self._get_jwt_token()

        # If the saved_rules_file option was used, check the value provided isn't a directory
        if saved_rules_file and os.path.isdir(saved_rules_file):
            logger.error("Please specify a file name for the saved rules file: '%s' is a directory", saved_rules_file)
            sys.exit(1)

        # Retrieve the rules from the specified URL or file
        if self.rules_location.startswith('http'):
            self.rules_file = saved_rules_file  # Will be None if saved_rules_file option wasn't used
            self.download_rules()
        else:
            # assume a file on the local system and check it exists
            self.rules_file = os.path.abspath(self.rules_location)
            if not os.path.isfile(self.rules_file):
                logger.error("Couldn't find rules file '%s'", self.rules_file)
                sys.exit(1)

        logger.debug("Using rules file %s", self.rules_file)

        # Get the rules to exclude, if any, as a comma separated string and convert to a list
        self.exclude_rules = exclude_rules if exclude_rules else self._get_config_option('client', 'exclude_rules', '')
        self.exclude_rules = self.exclude_rules.replace(',', ' ').split()
        if self.exclude_rules:
            logger.info("Rules to exclude: %s", ', '.join(self.exclude_rules))

        # Detect if the rules file is a text or binary (compiled) file (or otherwise)
        output = run_cmd(['/usr/bin/file', '-b', self.rules_file])
        rule_type = output.lower()
        if os.path.getsize(self.rules_file) == 0 or rule_type == 'empty':
            logger.error("Rules file '%s' is empty", self.rules_file)
            sys.exit(1)
        self.compiled_rules_flag = '-C' if rule_type.startswith('yara') or rule_type == 'data' else ''
        logger.debug("Rules file type: '%s', Compiled rules: %s", rule_type, self.compiled_rules_flag == '-C')

        # Quickly test the rules file to make sure it contains usable rules!
        # Note, if the compiled_rules_flag is '' it must be removed from the list or it causes problems
        cmd = list(filter(None, [self.yara_binary, '--fail-on-warnings', '-p', '1', '-f',
                                 self.compiled_rules_flag, self.rules_file, '/dev/null']))
        try:
            stdout, stderr = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                                              universal_newlines=True).communicate()
            if stderr:
                raise Exception(stderr)
        except Exception as err:
            error = str(err).strip()
            logger.error("Unable to use rules file '%s': %s", self.rules_file, error)
            sys.exit(1)

        # Limit the number of threads used by yara to limit the CPU load of the scans
        # If system has 2 or fewer CPUs, then use just one thread
        self.cpu_thread_limit = self._get_config_option('yara', 'cpu_thread_limit', 2)
        nproc = run_cmd(['/usr/bin/nproc'])
        if not nproc or int(nproc) <= 2:
            self.cpu_thread_limit = 1
        logger.debug("Using %d CPU thread(s) for scanning", self.cpu_thread_limit)

        self.nice = int(self._get_config_option('yara', 'nice', 19))
        self.nice_binary = '/bin/nice' if os.path.isfile('/bin/nice') else '/usr/bin/nice'
        self.scan_timeout = int(self._get_config_option('yara', 'scan_timeout', 3600))

        # Construct the (partial) yara command that will be used later for scanning files and processes
        # The argument for the files and processes that will be scanned will be added later
        self.yara_cmd = list(filter(None, [self.nice_binary, '-n', str(self.nice), self.yara_binary, '-s', '-N',
                                           '-a', str(self.scan_timeout), '-p', str(self.cpu_thread_limit),
                                           '-r', self.compiled_rules_flag, self.rules_file]))
        logger.debug("Yara command: %s", self.yara_cmd)

        # Dictionary in which to store all the scan matches.  Structure is like ...
        # host_scan = {rule_name: [{source: ..., stringData: ..., stringIdentifier: ..., stringOffset: ...},
        #                          {source: ...}],
        #              rule_name: [{...}, {...}, {...}],
        #              rule_name: [{...}, {...}, {...}],
        #              ... }
        self.host_scan = {}
        self.host_scan_mutation = ''

        if not hasattr(self, 'do_filesystem_scan'):
            self.do_filesystem_scan = string_to_bool(self._get_config_option('client', 'scan_filesystem', 'True'))
        if not hasattr(self, 'do_process_scan'):
            self.do_process_scan = string_to_bool(self._get_config_option('client', 'scan_processes', 'True'))
        self.metadata = string_to_bool(self._get_config_option('client', 'metadata', 'True'))
        self.string_match_limit = int(self._get_config_option('client', 'string_match_limit', 10))

    def download_rules(self):
        """
        Rules can be downloaded a couple of ways:
        - From a GraphQL supplying the rule body in rawRule
        - A file containing either compiled or plain text rule(s)
        """
        import shutil

        # File into which the rules are saved
        self.temp_rules_file = NamedTemporaryFile(prefix='tmp_malware-detection-client_rules.', mode='wb', delete=True)

        if self.rules_location.endswith('.yar'):
            # Assume a rules file and download it
            logger.debug("Downloading rules from: %s", self.rules_location)
            response = self.session.get(self.rules_location)
            if response.status_code != 200:
                logger.error("%s: %s", response.status_code, response.text)
                sys.exit(1)
            self.temp_rules_file.write(response.content)
        else:
            # get rules from GraphQL backend, ignoring any disabled ones
            rules_query = """
            query RulesQuery {
                rules(condition: {isDisabled: false}) {
                    nodes {
                        id
                        name
                        rawRule
                    }
                }
            }"""
            response = self.session.post(self.rules_location, json={'query': rules_query})
            if response.status_code != 200:
                logger.error("%s: %s", response.status_code, response.text)
                sys.exit(1)

            data = response.json()
            logger.debug(json.dumps(data, indent=2))

            # Do a quick test run of each rule
            # Ignore any that cause errors or warnings or are explicitly excluded
            rule_testing_file = NamedTemporaryFile(mode='wb')
            good_rules, problem_rules = {}, {}
            for rule in data['data']['rules']['nodes']:
                if any([exclude_rule.lower() in rule['name'].lower() for exclude_rule in self.exclude_rules]):
                    problem_rules[rule['name']] = rule['id']
                    continue
                if not rule['rawRule']:
                    problem_rules[rule['name']] = rule['id']
                    continue

                # Write rule to a temporary file and run it to make sure it runs without error
                rule_testing_file.seek(0)
                rule_testing_file.write(rule['rawRule'].encode('utf-8'))
                rule_testing_file.truncate()
                rule_testing_file.flush()
                cmd = [self.yara_binary, '--fail-on-warnings', '-p', '1', '-f', rule_testing_file.name, '/dev/null']
                try:
                    subprocess.Popen(cmd, universal_newlines=True)
                    logger.debug("Rule '%s' ok, adding", rule['name'])
                    good_rules[rule['name']] = rule['id']
                    self.temp_rules_file.write(rule['rawRule'].encode('utf-8'))
                except subprocess.CalledProcessError as e:
                    logger.warning("Rule '%s' failed: '%s', skipping ...", rule['name'], e.output)
                    problem_rules[rule['name']] = rule['id']
                    continue

            logger.debug("Rules to use ...")
            logger.debug(good_rules)
            logger.debug("Rules that won't be used ...")
            logger.debug(problem_rules)

        self.temp_rules_file.flush()
        # if -S/--save option used, copy the temp file to the user specified file and use that file
        if self.rules_file:
            logger.info("Saving rules to %s", self.rules_file)
            shutil.copy(self.temp_rules_file.name, self.rules_file)
            self.temp_rules_file.close()
        else:
            self.rules_file = self.temp_rules_file.name

    def parse_scan_output(self, output, exclude_items=[]):
        if not output:
            return

        # Each 'set' of output lines consists of 1 line containing the rule and file/pid (aka source) it matches
        # Followed by one or more related lines of matching string data from that source, eg
        # ...
        # rule_name source                            + Set of 3 related lines
        # 0x_offset:string_identifier:string_data     |
        # 0x_offset:string_identifier:string_data     +
        # rule_name source                            + Set of 2 related lines
        # 0x_offset:string_identifier:string_data     +
        # ...

        def skip_string_data_lines(string_data_lines):
            # Skip the 0x... lines containing string match data
            while string_data_lines and string_data_lines[0].startswith('0x'):
                logger.debug("Skipping string data line '%s'", string_data_lines[0])
                string_data_lines.pop(0)

        output_lines = output.split("\n")
        while output_lines:
            if 'error scanning ' in output_lines[0]:
                if output_lines[0].endswith('error: 4'):
                    # ERROR_COULD_NOT_MAP_FILE - only display this yara error if debugging (spammy)
                    logger.debug(output_lines[0])
                else:
                    logger.info(output_lines[0])
                output_lines.pop(0)  # Skip the error scanning line
                # Skip any string match lines after the error scanning line
                skip_string_data_lines(output_lines)
                continue
            # Get the rule_name and source from the first line in the set
            try:
                rule_name, source = output_lines[0].rstrip().split(" ", 1)
            except ValueError as err:
                # Hopefully shouldn't happen but log it and continue processing
                logger.info("Error parsing rule match '%s': %s", output_lines[0], err)
                output_lines.pop(0)  # Skip the erroneous line
                # Skip any string match lines afterwards until we get to the next rule match line
                skip_string_data_lines(output_lines)
                continue

            # All good so far, skip over the line containing the rule name and matching source file/pid
            output_lines.pop(0)

            # If the rule or the source file/pid is to be excluded, then skip over its scan matches
            # and move onto the next match
            if any([exclude_rule.lower() in rule_name.lower() for exclude_rule in self.exclude_rules]) \
                    or any([ei in source for ei in exclude_items]):
                skip_string_data_lines(output_lines)
                continue

            # Check if the rule name contains a ':' or doesn't start with a char/string
            # It shouldn't and its likely to be due to a malformed string_offset line
            # Skip any further scan matches until the next rule match
            if ':' in rule_name or not re.match('^[a-zA-Z]+', rule_name):
                skip_string_data_lines(output_lines)
                continue

            rule_match = {'rule_name': rule_name, 'matches': []}
            source_type = "process" if source.isdigit() else "file"

            # Parse the string match data for the remaining lines in the set
            string_matches = 0
            while output_lines and output_lines[0].startswith('0x'):
                if string_matches < self.string_match_limit:
                    try:
                        string_offset, string_identifier, string_data = output_lines[0].split(':', 2)
                        string_offset = int(string_offset, 0)
                    except ValueError as err:
                        logger.info("Error parsing string match '%s': %s", output_lines[0], err)
                        output_lines.pop(0)
                        continue
                    rule_match_dict = {'source': source,
                                       'string_data': string_data.strip(),
                                       'string_identifier': string_identifier,
                                       'string_offset': string_offset,
                                       'metadata': {'source_type': source_type}}
                    rule_match['matches'].extend([rule_match_dict])
                output_lines.pop(0)
                string_matches += 1

            # If string_match_limit is 0 or there was no string data, there will be no rule_matches,
            # but still record the file/pid source that was matched
            if not rule_match['matches']:
                rule_match_dict = {'source': source,
                                   'string_data': '',
                                   'string_identifier': '',
                                   'string_offset': -1,
                                   'metadata': {'source_type': source_type}}
                rule_match['matches'] = [rule_match_dict]

            if self.metadata:
                # Add extra data to each rule match, beyond what yara provides
                # Eg, for files: line numbers & context, checksums; for processes: process name
                # TODO: find more pythonic ways of doing this stuff instead of using system commands
                metadata_func = self.add_file_metadata if source_type == 'file' else self.add_process_metadata
                metadata_func(rule_match['matches'])

            self.matches += 1
            logger.info("Matched rule %s in %s %s", rule_name, source_type, source)
            logger.debug(rule_match)
            if self.host_scan.get(rule_match['rule_name']):
                self.host_scan[rule_match['rule_name']].extend(rule_match['matches'])
            else:
                self.host_scan[rule_match['rule_name']] = rule_match['matches']

    def add_process_metadata(self, rule_matches):
        """
        Add extra data to the process scan matches beyond what is provided by yara, eg process name
        """
        # All passed in rule_matches will have the same source PID
        # Check the process still exists before obtaining the metadata about it
        source = rule_matches[0]['source']
        if not os.path.exists('/proc/%s' % source):
            return

        # Get name of process from ps command
        # -h: no output header, -q: only the specified process, -o args: just the process name and args
        process_name = run_cmd(['/bin/ps', '-hq', source, '-o', 'args'])

        for rule_match in rule_matches:
            rule_match['metadata'].update({'process_name': process_name if process_name else 'unknown'})

    def add_file_metadata(self, rule_matches):
        """
        Add extra data to the file scan matches beyond what is provided by yara
        - eg matching line numbers, line context, file checksum
        - Use grep to get the line numbers & sed to get the line
        """
        def get_line_from_file(file_name, line_number):
            # Extract the line at line_number from file_name
            line_length_limit = 120
            line = run_cmd(['/bin/sed', '%dq;d' % line_number, file_name])
            # Limit line length if necessary and urlencode it to minimize problems with GraphQL when uploading
            return urlencode(line if len(line) < line_length_limit else line[:line_length_limit] + "...")

        # All passed in rule_matches will have the same source file
        # Check the file still exists before obtaining the metadata about it
        source = rule_matches[0]['source']
        if not os.path.exists(source):
            return

        # Get the file type, mime type and md5sum hash of the source file
        file_type = run_cmd(['/usr/bin/file', '-b', source])
        mime_type = run_cmd(['/usr/bin/file', '-bi', source])
        md5sum = run_cmd(['/usr/bin/md5sum', source]).split()[0]

        grep_string_data_match_list = []
        if mime_type and 'charset=binary' not in mime_type:
            # Get the line numbers for each of yara's string_data matches in the source file, but not for binary files
            # Build a grep command that searches for each of the string_data patterns in the source file
            # For each string_data pattern that grep finds, the grep output will have the form...
            # line_number:offset_from_0:string_data_pattern

            # Get the set of patterns to grep for, eg ['pattern1', 'pattern2', etc], ie remove duplicate patterns
            grep_string_data_pattern_set = set([match['string_data'] for match in rule_matches])
            if grep_string_data_pattern_set:
                # Build an option list for grep, eg ['-e', 'pattern1', '-e', 'pattern2', ... etc]
                # zip creates a list of tuples, eg [('-e', 'pattern'), ('-e', 'pattern2'), ...], then flatten the list
                grep_string_data_patterns = [item for tup in list(zip(['-e'] * len(grep_string_data_pattern_set),
                                                                      grep_string_data_pattern_set))
                                             for item in tup]
                # Create the grep command to execute.  -F means don't interpret regex special chars in the patterns
                grep_command = ['/bin/grep', '-Fbon'] + grep_string_data_patterns + [source]
                logger.debug("grep command: %s", grep_command)
                grep_output = run_cmd(grep_command)

                # Now turn the grep output into a list of tuples for easier searching a little later, ie
                # [(line_number, offset_from_0, string_data_pattern), (...), ]
                if grep_output and not grep_output.lower().startswith('binary'):
                    grep_string_data_match_list = list(map(lambda grep_output_line: tuple(grep_output_line.split(':', 3)),
                                                           grep_output.splitlines()))

        for rule_match in rule_matches:
            metadata = rule_match['metadata']
            metadata.update({'file_type': file_type,
                             'mime_type': mime_type,
                             'md5sum': md5sum})
            if grep_string_data_match_list:
                # Now, for each offset_from_0 in the grep output, we want to match it with the corresponding
                # string_offset value from the yara output so we can get the line number for that string_data match
                # And while we are here, get the line from the source file at that line number
                line_number = None
                for grep_list_item in grep_string_data_match_list:
                    if int(grep_list_item[1]) == rule_match['string_offset']:
                        line_number = int(grep_list_item[0])
                        break
                if line_number:
                    metadata.update({'line_number': line_number,
                                     'line': get_line_from_file(source, line_number)})

    def scan_filesystem(self):
        if not self.do_filesystem_scan:
            return False

        scan_dict = process_items(include_items=self.scan_fsobjects,
                                  exclude_items=[] if self.rules_file in self.scan_fsobjects else [self.rules_file])
        logger.info("Filesystem objects to be scanned in: %s", sorted(scan_dict.keys()))

        logger.info("Starting filesystem scan ...")
        fs_scan_start = time.time()

        for toplevel_dir in sorted(scan_dict):
            # Make a copy of the self.yara_cmd list and add to it the thing to scan
            cmd = self.yara_cmd[:]
            scan_start = time.time()

            if self.scan_since['timestamp']:
                logger.info("Scanning files in '%s' modified since %s ...", toplevel_dir, self.scan_since['datetime'])
                # Find the recently modified files in the given top level directory
                scan_list_file = NamedTemporaryFile(prefix='%s_scan_list.' % toplevel_dir[1:], mode='w', delete=True)
                if 'include' in scan_dict[toplevel_dir]:
                    find_modified_include_items(scan_dict[toplevel_dir]['include'], self.scan_since['timestamp'], scan_list_file)
                else:
                    find_modified_in_directory(toplevel_dir, self.scan_since['timestamp'], scan_list_file)

                scan_list_file.flush()
                cmd.extend(['--scan-list', scan_list_file.name])
            else:
                logger.info("Scanning files in '%s' ...", toplevel_dir)
                if 'include' in scan_dict[toplevel_dir]:
                    scan_list_file = NamedTemporaryFile(prefix='%s_scan_list.' % toplevel_dir[1:], mode='w', delete=True)
                    scan_list_file.write('\n'.join(scan_dict[toplevel_dir]['include']))
                    scan_list_file.flush()
                    cmd.extend(['--scan-list', scan_list_file.name])
                else:
                    cmd.append(toplevel_dir)

            logger.debug("Yara command: %s", ' '.join(cmd))
            try:
                stdout, stderr = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                                                  universal_newlines=True).communicate()
                scan_output = stdout.strip()
            except subprocess.CalledProcessError as err:
                logger.error("Unable to scan %s: %s", toplevel_dir, err.output.strip())
                continue

            self.parse_scan_output(scan_output)

            scan_end = time.time()
            logger.info("Scan time for '%s': %d seconds", toplevel_dir, (scan_end - scan_start))

        fs_scan_end = time.time()
        logger.info("Filesystem scan time: %s", time.strftime("%H:%M:%S", time.gmtime(fs_scan_end - fs_scan_start)))
        return True

    def scan_processes(self):
        if not self.do_process_scan:
            return False

        logger.info("Starting processes scan ...")
        pids_scan_start = time.time()

        if not self.scan_pids:
            # Get list of process ids to scan
            all_pids = [entry for entry in os.listdir('/proc') if entry.isdigit()]
            exclude_pids = [str(os.getpid())]  # exclude our script's pid at least
            self.scan_pids = sorted(list(set(all_pids) - set(exclude_pids)), key=lambda pid: int(pid))

        for scan_pid in self.scan_pids:
            scan_start = time.time()
            logger.info("Scanning process '%s' ...", scan_pid)
            cmd = self.yara_cmd + [str(scan_pid)]
            logger.debug("Yara command: %s", ' '.join(cmd))
            try:
                stdout, stderr = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                                                  universal_newlines=True).communicate()
                scan_output = stdout.strip()
            except subprocess.CalledProcessError:
                # Usually if the user doesn't have permissions to scan the process
                continue

            self.parse_scan_output(scan_output)

            scan_end = time.time()
            logger.info("Scan time for process '%s': %d seconds", scan_pid, (scan_end - scan_start))

        pids_scan_end = time.time()
        logger.info("Processes scan time: %s", time.strftime("%H:%M:%S", time.gmtime(pids_scan_end - pids_scan_start)))
        return True

    def upload_host_scan(self):
        if self.matches == 0:
            logger.info("No matches found")
        elif not self.no_upload:
            logger.info("Uploading scan results of %d match%s to %s",
                        self.matches, 'es' if self.matches > 1 else '', self.results_url)

        self.host_scan_mutation = self.create_host_scan_mutation()
        logger.debug("Scan results ..." + self.host_scan_mutation)
        if self.no_upload:
            logger.info('Skipping scan results upload because of --no-upload option')
            return

        # If using JWT tokens and the scan took a long time, the token may have expired.  Request a new one in that case
        if self.session.cookies.get('cs_jwt'):
            self._get_jwt_token()

        # Upload the results/mutation to the backend
        response = self.session.post(self.results_url, json={'query': self.host_scan_mutation})
        if response.status_code != 200:
            logger.error("Uploading results failed: %s: %s", response.status_code, response.text)
            sys.exit(1)
        else:
            # All responses, including errors, always return HTTP 200 OK status codes.
            # An error response will contain an "errors" field.
            errors = response.json().get('errors')
            if errors:
                logger.error('Uploading results failed: %s.  %s', errors[0].get('message'), errors[0].get('hint'))
                sys.exit(1)
            if self.test_option:
                logger.info("Test scan results uploaded successfully.  "
                            "Note that test scan results will not appear in the UI")
            else:
                logger.info("Scan results uploaded successfully.")

    def create_host_scan_mutation(self):
        # Build the mutation text
        mutation_header = """
        mutation HostScan {
          recordHostScan(
            input: {
              scannedhost: {
                insightsId: "%s"
                rulesScanned: [""" % self.insights_id

        mutation_footer = """
                ]
              }
            }
          ) {
            success
          }
        }"""

        mutation = mutation_header
        for rule_name in self.host_scan.keys():
            rule_scan = """{
                ruleName: "%s"
                stringsMatched: [""" % rule_name
            for match in self.host_scan[rule_name]:
                rule_scan += """{
                    source: "%s"
                    stringData: %s
                    stringIdentifier: %s
                    stringOffset: "%s"
                    metadata: "%s"
                }, """ % (match['source'],
                          json.dumps(match['string_data']),
                          json.dumps(match['string_identifier']),
                          match['string_offset'],
                          json.dumps(match['metadata']).replace('"', '\\"'))
            rule_scan += "]}, "
            mutation += rule_scan

        mutation += mutation_footer
        return mutation

    def run(self):
        if os.geteuid() != 0:
            logger.warning("Run this script via sudo/as root to completely scan your system")
            # sys.exit(1)

        self.scan_filesystem()
        self.scan_processes()
        if self.do_filesystem_scan or self.do_process_scan:
            self.upload_host_scan()
        else:
            logger.error("No scans performed, nothing to upload")
            sys.exit(1)

    def _get_config_option(self, section, option, default_value=None):
        """
        Get the value of a configuration option or, if it doesn't exist, the default_value
        """
        try:
            return self.cfg.get(section, option)
        except Exception:
            logger.debug("Using default value of %s for config item [%s][%s]" % (str(default_value), section, option))
            return default_value

    def _get_jwt_token(self):
        """
        Rules &/or results endpoints may require a Keycloak JWT token to access
        The specific auth values are retrieved from in the config file
        Then the JWT token is passed to the backend with every request
        """
        from .jwt_utils import load_jwt_token, save_jwt_token
        from hashlib import md5

        keycloak_url = self._get_config_option("auth", "keycloak_url")
        if keycloak_url:
            logger.debug("Using JWT token auth ...")
            cookie = load_jwt_token()
            if cookie and md5(keycloak_url.encode('utf-8')).hexdigest() == cookie['auth_server']:
                self.session.cookies.update(cookie)
                return

            logger.info("Retrieving JWT token from %s", keycloak_url)
            username = self._get_config_option("auth", "username")
            password = self._get_config_option("auth", "password")
            client_id = self._get_config_option("auth", "client_id")
            auth_data = {'client_id': client_id, 'grant_type': 'password', 'scope': 'openid', 'username': username, 'password': password}
            missing_values = sorted([k for k, v in auth_data.items() if auth_data[k] is None], reverse=True)
            if any(missing_values):
                logger.error("Missing authentication value for %s", ', '.join(missing_values))
                sys.exit(1)
            response = self.session.post(keycloak_url, data=auth_data)
            if response.status_code != 200:
                logger.error("%s: %s", response.status_code, response.text)
                sys.exit(1)
            data = response.json()
            cookie = {'auth_server': md5(keycloak_url.encode('utf-8')).hexdigest(),
                      'cs_jwt': data['id_token'],
                      'cs_jwt_refresh': data['refresh_token']}
            self.session.cookies.update(cookie)
            save_jwt_token(cookie)

    def _get_cert_auth(self):
        logger.debug("Using cert auth ...")
        cert = self._get_config_option('auth', 'cert')
        key = self._get_config_option('auth', 'key')
        if not (cert and key):
            logger.error("No certificate and/or key file specified for authentication in the configuration file '%s'.  Please add them.",
                         self.conf_file)
            sys.exit(1)
        if not os.access(cert, os.R_OK):
            logger.error("Unable to read certificate file '%s'. "
                         "Ensure it exists and have permission to read it (eg you're running via sudo/as root)", cert)
            sys.exit(1)
        if not os.access(key, os.R_OK):
            logger.error("Unable to read key file '%s'. "
                         "Ensure it exists and have permission to read it (eg you're running via sudo/as root)", key)
            sys.exit(1)

        self.session.cert = (cert, key)

    @staticmethod
    def _get_test_rule_location(rules_location):
        test_rule = 'test-rule.yar'
        # Nothing to do if the location already ends with test_rule
        if rules_location.endswith(test_rule):
            return rules_location
        # if rules location is a URL or directory that doesn't end in /, then add a '/'
        if (re.match('^https?://', rules_location) and not urlparse(rules_location).path) or\
                (os.path.isdir(rules_location) and not rules_location.endswith('/')):
            rules_location += '/'

        # Just replace the value after the last '/' with test_rule
        # TODO: a bit more intelligence can probably be applied for adding the test rule
        return rules_location.rsplit('/', 1)[0] + '/' + test_rule
