#!/usr/libexec/platform-python
import json
import os
import sys
import time
import requests
import subprocess
import logging
from iniparse import RawConfigParser
import re
import urllib3
try:
    from urllib import quote as urlencode
except ImportError:
    from urllib.parse import quote as urlencode

from .utils import (
    load_jwt_token, save_jwt_token,
    get_insights_id,
    create_new_conf_file,
    run_cmd, string_to_bool,
    logger
)

__version__ = "0.0.4"
MIN_YARA_VERSION = "4.1.0"
CONF_DIR = "/etc/malware-detection-client/"
CONF_FILE = CONF_DIR + "malware-detection-client.conf"


class MalwareDetectionClient:

    def __init__(self,
                 rules_location='',
                 results_url='',
                 scan_entity=None,
                 saved_rules_file=None,
                 exclude_rules='',
                 conf_file=CONF_FILE,
                 new_conf_file=None,
                 no_upload=False,
                 debug=None):

        self.matches = 0
        self.no_upload = no_upload

        self.conf_file = conf_file
        if new_conf_file:
            create_new_conf_file(new_conf_file, rules_location, results_url)

        self.cfg = RawConfigParser()
        if self.conf_file:
            if os.path.isfile(self.conf_file):
                self.cfg.read(self.conf_file)
            else:
                logger.warning("Couldn't find specified config file '%s', using default values", self.conf_file)

        if debug:
            log_level = 'DEBUG'
        elif os.getenv('LOG_LEVEL'):
            log_level = os.getenv('LOG_LEVEL').upper()
        else:
            log_level = logging.getLevelName(self._get_config_option('client', 'log_level', 'INFO').upper())
        logger.setLevel(log_level)
        logging.getLogger('requests').setLevel(log_level)

        # Find the yara binary on the local system
        self.yara_binary = self._get_config_option('yara', 'location', None)
        if not self.yara_binary:
            try:
                self.yara_binary = subprocess.check_output("which yara 2>/dev/null", shell=True).strip().decode("utf-8")
            except subprocess.CalledProcessError:
                self.yara_binary = "/usr/local/bin/yara"
        if not os.path.isfile(self.yara_binary):
            logger.error("Couldn't find yara.  Please ensure it is installed and in the search path")
            sys.exit(1)
        logger.debug("Using yara binary: %s", self.yara_binary)

        # Check the installed yara version >= MIN_YARA_VERSION
        installed_yara_version = subprocess.check_output("yara --version", shell=True).strip()
        try:
            if float(installed_yara_version[:3]) < float(MIN_YARA_VERSION[:3]):
                logger.error("Found yara version %s, but malware-detection-client needs version >= %s"
                             % (installed_yara_version, MIN_YARA_VERSION))
                sys.exit(1)
        except (IndexError, ValueError):
            # Either the indexing or converting to float went wrong.  Keep going anyway
            pass

        # Check if the -N / symlink option is available
        self.symlink_option = '-N'
        cmd = "%s --help | grep -- '%s'> /dev/null" % (self.yara_binary, self.symlink_option)
        try:
            subprocess.check_output(cmd, shell=True)
            logger.debug("Using the --no-follow-symlinks option: '%s'", self.symlink_option)
        except subprocess.CalledProcessError:
            self.symlink_option = ''

        # Get thing to scan from passed in args, if any
        if scan_entity:
            if os.path.exists(scan_entity):
                if re.match('^/[/.]*$', scan_entity):
                    # root directory '/' was specified (not recommended), so expand it to a list of subdirectories
                    self.scan_fsobjects = list(map(lambda x: "/" + x, os.listdir('/')))
                else:
                    self.scan_fsobjects = [scan_entity]
                self.do_filesystem_scan = True
                self.do_process_scan = False
                logger.info("Scan argument: %s %s", "directory" if os.path.isdir(scan_entity) else "file", scan_entity)
            elif os.path.exists('/proc/' + scan_entity):
                self.scan_pids = [int(scan_entity)]
                self.do_filesystem_scan = False
                self.do_process_scan = True
                logger.info("Scan argument: process %s", scan_entity)
            else:
                logger.error("Couldn't find the specified file/directory/process id to scan: %s.  Exiting", scan_entity)
                sys.exit(1)

        self.nice = int(self._get_config_option('yara', 'nice', 19))

        # Get the location of the rules, ie a file or URL, or exit if not specified
        self.rules_location = rules_location if rules_location else self._get_config_option('client', 'rules_location', '')
        if not self.rules_location:
            logger.error("Please specify a location from which to get the rules")
            sys.exit(1)
        logger.info("Getting the rules from %s", self.rules_location)

        # Get the location where the scan results are to be uploaded, or exit if not specified
        self.results_url = results_url if results_url else self._get_config_option('client', 'results_location', '')
        if self.no_upload:
            logger.info("Scan results will not be uploaded because of --no-upload option")
        else:
            if not self.results_url:
                logger.error("Please specify a location to which to upload the scan results")
                sys.exit(1)
            logger.debug("Scan results will be uploaded to %s", self.results_url)

        # If we are either downloading rules or uploading results via http then create a requests session
        # - set the http proxy if defined
        # - set the auth cookie (JWT token) if keycloak_url is defined
        if self.rules_location.startswith('http') or self.results_url.startswith('http'):
            self.session = requests.Session()
            proxy = self._get_config_option('client', 'proxy_url', None)
            self.session.proxies.update({'http': proxy, 'https': proxy} if proxy else {})
            if self._get_config_option('auth', 'keycloak_url', None):
                self._get_jwt_token()

        # Retrieve the rules from the specified URL or file
        if self.rules_location.startswith('http'):
            self.rules_file = saved_rules_file
            self.download_rules()
        else:
            # assume a file on the local system
            self.rules_file = self.rules_location
            if not os.path.isfile(self.rules_file):
                logger.error("Couldn't find rules file '%s', exiting", self.rules_file)
                sys.exit(1)

        logger.debug("Using rules file %s", self.rules_file)

        # Get the rules to exclude, if any, as a comma separated string and convert to a list
        self.exclude_rules = exclude_rules if exclude_rules else self._get_config_option('client', 'exclude_rules', '')
        self.exclude_rules = self.exclude_rules.replace(',', ' ').split()
        if self.exclude_rules:
            logger.info("Rules to exclude: %s", ', '.join(self.exclude_rules))

        # Detect if the rules file is a text or binary (compiled) file (or otherwise)
        cmd = "file -b %s" % self.rules_file
        rule_type = subprocess.check_output(cmd, shell=True).decode('utf-8').strip().lower()
        if os.path.getsize(self.rules_file) == 0 or rule_type == 'empty':
            logger.error("Rules file is empty, exiting")
            sys.exit(1)
        self.compiled_rules_flag = '-C' if rule_type.startswith('yara') or rule_type == 'data' else ''
        logger.debug("Rules file type: '%s', Compiled rules: %s", rule_type, self.compiled_rules_flag == '-C')

        # Quickly test the rules file to make sure it contains usable rules!
        cmd = "nice -n %d %s --fail-on-warnings -p 1 -f %s %s /dev/null" % \
              (self.nice, self.yara_binary, self.compiled_rules_flag, self.rules_file)
        try:
            subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT)
        except subprocess.CalledProcessError as err:
            error = err.output.strip().decode('utf-8')
            logger.error("Unable to use rule file '%s': %s", self.rules_file, error)
            sys.exit(1)

        # Limit the number of threads to limit the CPU load of the scans
        # If system has 2 or fewer CPUs, then use just one thread
        self.cpu_thread_limit = self._get_config_option('yara', 'cpu_thread_limit', 2)
        nproc = int(subprocess.check_output("nproc", shell=True).strip().decode('utf-8'))
        if nproc <= 2:
            self.cpu_thread_limit = 1
        logger.debug("CPU has %d threads, using %d threads for scanning", nproc, self.cpu_thread_limit)

        self.scan_timeout = int(self._get_config_option('yara', 'scan_timeout', 3600))
        self.string_match_limit = int(self._get_config_option('client', 'string_match_limit', 10))

        # Dictionary in which to store all the scan matches.  Structure is like ...
        # host_scan = {rule_name: [{source: ..., stringData: ..., stringIdentifier: ..., stringOffset: ...},
        #                          {source: ...}],
        #              rule_name: [{...}, {...}, {...}],
        #              rule_name: [{...}, {...}, {...}],
        #              ... }
        self.host_scan = {}
        self.host_scan_mutation = ''

        if not hasattr(self, 'do_filesystem_scan'):
            self.do_filesystem_scan = string_to_bool(self._get_config_option('client', 'scan_filesystem', 'True'))
        if not hasattr(self, 'do_process_scan'):
            self.do_process_scan = string_to_bool(self._get_config_option('client', 'scan_processes', 'True'))

    def _get_jwt_token(self):
        """
        Rules &/or results endpoints may require a Keycloak JWT token to access
        The specific auth values are retrieved from in the config file
        Then the JWT token is passed to the backend with every request
        """
        keycloak_url = self._get_config_option("auth", "keycloak_url", None)
        if keycloak_url:
            cookie = load_jwt_token()
            if cookie:
                self.session.cookies.update(cookie)
                return

            logger.info("Retrieving JWT token from %s", keycloak_url)
            username = self._get_config_option("auth", "username", None)
            password = self._get_config_option("auth", "password", None)
            client_id = self._get_config_option("auth", "client_id", None)
            auth_data = {'client_id': client_id, 'grant_type': 'password', 'scope': 'openid', 'username': username, 'password': password}
            missing_values = sorted([k for k, v in auth_data.items() if auth_data[k] is None], reverse=True)
            if any(missing_values):
                logger.error("Missing authentication value for %s", ', '.join(missing_values))
                sys.exit(1)
            response = self.session.post(keycloak_url, data=auth_data)
            if response.status_code != 200:
                logger.error("%s: %s", response.status_code, response.text)
                sys.exit(1)
            data = response.json()
            cookie = {'cs_jwt': data['id_token'], 'cs_jwt_refresh': data['refresh_token']}
            self.session.cookies.update(cookie)
            save_jwt_token(cookie)

    def download_rules(self):
        """
        Rules can be downloaded a couple of ways:
        - From a GraphQL supplying the rule body in rawRule
        - A file containing either compiled or plain text rule(s)
        """

        from tempfile import NamedTemporaryFile
        import shutil
        urllib3.disable_warnings()

        # File into which the rules are saved
        self.temp_rules_file = NamedTemporaryFile(prefix='tmp_malware-detection-client_rules.', mode='wb', delete=True)

        if self.rules_location.endswith('.yar'):
            # Assume a rules file and download it
            logger.debug("Downloading rules from: %s", self.rules_location)
            response = self.session.get(self.rules_location)
            if response.status_code != 200:
                logger.error("%s: %s", response.status_code, response.text)
                sys.exit(1)
            self.temp_rules_file.write(response.content)
        else:
            # get rules from GraphQL backend, ignoring any disabled ones
            rules_query = """
            query {
                rules(condition: {isDisabled: false}) {
                    nodes {
                        id
                        name
                        rawRule
                    }
                }
            }"""
            response = self.session.post(self.rules_location, json={'query': rules_query})
            if response.status_code != 200:
                logger.error("%s: %s", response.status_code, response.text)
                sys.exit(1)

            data = response.json()
            logger.debug(json.dumps(data, indent=2))

            # Do a quick test run of each rule
            # Ignore any that cause errors or warnings or are explicitly excluded
            rule_testing_file = NamedTemporaryFile(mode='wb')
            good_rules, problem_rules = {}, {}
            for rule in data['data']['rules']['nodes']:
                if any([exclude_rule.lower() in rule['name'].lower() for exclude_rule in self.exclude_rules]):
                    problem_rules[rule['name']] = rule['id']
                    continue
                if not rule['rawRule']:
                    problem_rules[rule['name']] = rule['id']
                    continue

                # Write rule to a temporary file and run it to make sure it runs without error
                rule_testing_file.seek(0)
                rule_testing_file.write(rule['rawRule'].encode('utf-8'))
                rule_testing_file.truncate()
                rule_testing_file.flush()
                cmd = "nice -n %d %s --fail-on-warnings -p 1 -f %s /dev/null" % \
                      (self.nice, self.yara_binary, rule_testing_file.name)
                try:
                    subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT)
                    logger.debug("Rule '%s' ok, adding", rule['name'])
                    good_rules[rule['name']] = rule['id']
                    self.temp_rules_file.write(rule['rawRule'].encode('utf-8'))
                except subprocess.CalledProcessError as e:
                    logger.warning("Rule '%s' failed: '%s', skipping ...", rule['name'], e.output)
                    problem_rules[rule['name']] = rule['id']
                    continue

            logger.debug("Rules to use ...")
            logger.debug(good_rules)
            logger.debug("Rules that won't be used ...")
            logger.debug(problem_rules)

        self.temp_rules_file.flush()
        # if -S/--save option used, copy the temp file to the user specified file and use that file
        if self.rules_file:
            logger.info("Saving rules to %s", self.rules_file)
            shutil.copy(self.temp_rules_file.name, self.rules_file)
            self.temp_rules_file.close()
        else:
            self.rules_file = self.temp_rules_file.name

    def parse_scan_output(self, output, exclude_items=[]):
        if not output:
            return

        # Each 'set' of output lines consists of 1 line containing the rule and file/pid (aka source) it matches
        # Followed by one or more related lines of matching string data from that source, eg
        # ...
        # rule_name source                            + Set of 3 related lines
        # 0x_offset:string_identifier:string_data     |
        # 0x_offset:string_identifier:string_data     +
        # rule_name source                            + Set of 2 related lines
        # 0x_offset:string_identifier:string_data     +
        # ...

        def skip_string_data_lines(string_data_lines):
            # Skip the 0x... lines containing string match data
            while string_data_lines and string_data_lines[0].startswith('0x'):
                string_data_lines.pop(0)

        output_lines = output.split("\n")
        while output_lines:
            if 'error scanning ' in output_lines[0]:
                if output_lines[0].endswith('error: 4'):
                    # ERROR_COULD_NOT_MAP_FILE - only display this yara error if debugging (spammy)
                    logger.debug(output_lines[0])
                else:
                    logger.info(output_lines[0])
                output_lines.pop(0)  # Skip the error scanning line
                # Skip any string match lines after the error scanning line
                skip_string_data_lines(output_lines)
                continue
            # Get the rule_name and source from the first line in the set
            try:
                rule_name, source = output_lines[0].strip().split(" ", 1)
            except ValueError as ve:
                # Hopefully shouldn't happen but log it and continue processing
                logger.error("Encountered '%s' processing line '%s'" % (ve, output_lines[0]))
                output_lines.pop(0)  # Skip the erroneous line
                # Skip any string match lines afterwards until we get to the next rule match line
                skip_string_data_lines(output_lines)
                continue

            # All good so far, skip over the line containing the rule name and matching source file/pid
            output_lines.pop(0)

            # If the rule or the source file/pid is to be excluded, then skip over its scan matches
            # and move onto the next match
            if any([exclude_rule.lower() in rule_name.lower() for exclude_rule in self.exclude_rules]) \
                    or any([ei in source for ei in exclude_items]):
                skip_string_data_lines(output_lines)
                continue

            rule_match = {'rule_name': rule_name, 'matches': []}
            source_type = "process" if source.isdigit() else "file"

            # Parse the string match data for the remaining lines in the set
            string_matches = 0
            while output_lines and output_lines[0].startswith('0x'):
                if string_matches < self.string_match_limit:
                    string_offset, string_identifier, string_data = output_lines[0].split(':', 2)
                    rule_match_dict = {'source': source,
                                       'string_data': string_data.strip(),
                                       'string_identifier': string_identifier,
                                       'string_offset': int(string_offset, 0),
                                       'metadata': {'source_type': source_type}}
                    rule_match['matches'].extend([rule_match_dict])
                output_lines.pop(0)
                string_matches += 1

            # If string_match_limit is 0 or there was no string data, there will be no rule_matches,
            # but still record the file/pid source that was matched
            if not rule_match['matches']:
                rule_match_dict = {'source': source,
                                   'string_data': '',
                                   'string_identifier': '',
                                   'string_offset': -1,
                                   'metadata': {'source_type': source_type}}
                rule_match['matches'] = [rule_match_dict]

            # Add extra data to each rule match, beyond what yara provides
            # Eg, for files: line numbers & context, checksums; for processes: process name
            # TODO: find more pythonic ways of doing this stuff instead of using system commands
            metadata_func = self.add_file_metadata if source_type == 'file' else self.add_process_metadata
            metadata_func(rule_match['matches'])

            self.matches += 1
            logger.info("Matched rule %s in %s %s", rule_name, source_type, source)
            logger.debug(rule_match)
            if self.host_scan.get(rule_match['rule_name']):
                self.host_scan[rule_match['rule_name']].extend(rule_match['matches'])
            else:
                self.host_scan[rule_match['rule_name']] = rule_match['matches']

    def add_process_metadata(self, rule_matches):
        """
        Add extra data to the process scan matches beyond what is provided by yara, eg process name
        """
        # All passed in rule_matches will have the same source PID
        # Check the process still exists before obtaining the metadata about it
        source = rule_matches[0]['source']
        if not os.path.exists('/proc/%s' % source):
            return

        # Get name of process from ps command
        # -h: no output header, -q: only the specified process, -o args: just the process name and args
        process_name = run_cmd("ps -hq %s -o args" % source)

        for rule_match in rule_matches:
            rule_match['metadata'].update({'process_name': process_name if process_name else 'unknown'})

    def add_file_metadata(self, rule_matches):
        """
        Add extra data to the file scan matches beyond what is provided by yara
        - eg matching line numbers, line context, file checksum
        - Use grep to get the line numbers & sed to get the line
        """
        def get_line_from_file(file_name, line_number):
            line_length_limit = 120
            sed_cmd = "sed '%dq;d' %s" % (line_number, file_name)
            line = run_cmd(sed_cmd)
            # Limit line length if necessary and urlencode it to minimize problems with GraphQL when uploading
            return urlencode(line if len(line) < line_length_limit else line[:line_length_limit] + "...")

        # All passed in rule_matches will have the same source file
        # Check the file still exists before obtaining the metadata about it
        source = rule_matches[0]['source']
        if not os.path.exists(source):
            return

        # Get the file type, mime type and md5sum hash of the source file
        source = source.replace(' ', '\\ ')  # Handle filenames with spaces in them
        file_type = run_cmd("file -b %s" % source)
        mime_type = run_cmd("file -bi %s" % source)
        md5sum = run_cmd("md5sum %s | cut -d' ' -f1" % source)

        grep_string_data_match_list = []
        if mime_type and 'charset=binary' not in mime_type:
            # Get the line numbers for each of yara's string_data matches in the source file, but not for binary files
            # Build a grep command that searches for each of the string_data patterns in the source file
            # For each string_data pattern that grep finds, the grep output will have the form...
            # line_number:offset_from_0:string_data_pattern
            grep_string_data_patterns = ' -e ' + ' -e '.join(['"' + rm['string_data'].replace('"', '\\"') + '"'
                                                              for rm in rule_matches])
            grep_output = run_cmd("grep -Ebon %s %s" % (grep_string_data_patterns, source))

            # Now turn the grep output into a list of tuples for easier searching a little later, ie
            # [(line_number, offset_from_0, string_data_pattern), (...), ]
            if grep_output and not grep_output.lower().startswith('binary'):
                grep_string_data_match_list = list(map(lambda grep_output_line: tuple(grep_output_line.split(':', 3)),
                                                       grep_output.splitlines()))

        for rule_match in rule_matches:
            metadata = rule_match['metadata']
            metadata.update({'file_type': file_type,
                             'mime_type': mime_type,
                             'md5sum': md5sum})
            if grep_string_data_match_list:
                # Now, for each offset_from_0 in the grep output, we want to match it with the corresponding
                # string_offset value from the yara output so we can get the line number for that string_data match
                # And while we are here, get the line from the source file at that line number
                line_number = None
                for grep_list_item in grep_string_data_match_list:
                    if int(grep_list_item[1]) == rule_match['string_offset']:
                        line_number = int(grep_list_item[0])
                        break
                if line_number:
                    metadata.update({'line_number': line_number,
                                     'line': get_line_from_file(source, line_number)})

    def get_fsobject_list(self, fsobject_list_file, defaults=[]):
        # Parse a file containing a list of filesystem objects to scan, one entry per line
        # Making sure the entries are all valid filesystem objects (files or directories)
        if not os.path.isfile(fsobject_list_file):
            logger.warning("Couldn't find '%s', using default values ...", fsobject_list_file)
            return defaults

        fsobject_list = []
        no_objects = True
        for line in open(fsobject_list_file):
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            # Get the first entry on the line and check it exists before adding it
            # If '/', then substitute a list of all the top level directories
            no_objects = False
            obj = line.split()[0]
            if os.path.exists(obj):
                if re.match('^/[/.]*$', obj):
                    # root directory '/' was specified, so expand it to a list of subdirectories
                    fsobject_list.extend(list(map(lambda x: "/" + x, os.listdir('/'))))
                else:
                    fsobject_list.append(obj)

        if no_objects:
            logger.warning("No filesystem objects specified in '%s', using default values ...", fsobject_list_file)
            return defaults

        # Remove any duplicates before returning
        return list(set(fsobject_list))

    def scan_filesystem(self):
        if not self.do_filesystem_scan:
            return False

        # Default directories to scan (and not scan) if none are specified by user (via cmdline or config file)
        default_fs_include = ['/bin', '/boot', '/etc', '/home', '/lib', '/lib64',
                              '/opt', '/root', '/run', '/tmp', '/usr', '/var']
        default_fs_exclude = ['/cgroup', '/dev', '/media', '/mnt', '/proc', '/selinux', '/sys']

        if not hasattr(self, 'scan_fsobjects') or not self.scan_fsobjects:
            # Get list of filesystem objects (files/directories) to scan
            fs_include = self.get_fsobject_list(CONF_DIR + 'filesystem_include.txt', defaults=default_fs_include)
            fs_exclude = self.get_fsobject_list(CONF_DIR + 'filesystem_exclude.txt', defaults=default_fs_exclude)
            self.scan_fsobjects = sorted(list(set(fs_include) - set(fs_exclude)))
        logger.debug("File system objects to scan: %s", ', '.join(self.scan_fsobjects))

        # Exclude the rules_file, unless that's the thing you specifically wanted to scan
        exclude_files = [] if self.rules_file == ''.join(self.scan_fsobjects) else [self.rules_file]

        logger.info("Starting filesystem scan ...")
        fs_scan_start = time.time()

        for scan_fsobject in self.scan_fsobjects:
            scan_start = time.time()
            logger.info("Scanning %s ...", scan_fsobject)
            cmd = "nice -n %d %s -s %s -a %d -p %d -r %s %s %s" % \
                  (self.nice, self.yara_binary, self.symlink_option, self.scan_timeout, self.cpu_thread_limit,
                   self.compiled_rules_flag, self.rules_file, scan_fsobject)
            scan_output = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT).decode('utf-8').strip()

            self.parse_scan_output(scan_output, exclude_items=exclude_files)

            scan_end = time.time()
            logger.info("Scan time for %s: %d seconds", scan_fsobject, (scan_end - scan_start))

        fs_scan_end = time.time()
        logger.info("Filesystem scan time: %s", time.strftime("%H:%M:%S", time.gmtime(fs_scan_end - fs_scan_start)))
        return True

    def scan_processes(self):
        if not self.do_process_scan:
            return False

        logger.info("Starting processes scan ...")
        pids_scan_start = time.time()

        if not hasattr(self, 'scan_pids') or not self.scan_pids:
            # Get list of process ids to scan
            all_pids = [entry for entry in os.listdir('/proc') if entry.isdigit()]
            exclude_pids = [str(os.getpid())]  # exclude our script's pid at least
            self.scan_pids = sorted(list(set(all_pids) - set(exclude_pids)), key=lambda pid: int(pid))

        for scan_pid in self.scan_pids:
            scan_start = time.time()
            logger.info("Scanning process %s ...", scan_pid)
            cmd = "nice -n %d %s -s %s -a %d -p %d -r %s %s %s" % \
                (self.nice, self.yara_binary, self.symlink_option, self.scan_timeout, self.cpu_thread_limit,
                 self.compiled_rules_flag, self.rules_file, scan_pid)
            try:
                scan_output = subprocess.check_output(cmd, shell=True).decode('utf-8').strip()
            except subprocess.CalledProcessError:
                # Usually if the user doesn't have permissions to scan the process
                continue

            self.parse_scan_output(scan_output)

            scan_end = time.time()
            logger.info("Scan time for process %s: %d seconds", scan_pid, (scan_end - scan_start))

        pids_scan_end = time.time()
        logger.info("Processes scan time: %s", time.strftime("%H:%M:%S", time.gmtime(pids_scan_end - pids_scan_start)))
        return True

    def upload_host_scan(self):
        if self.matches == 0:
            logger.info("No matches found")
        elif not self.no_upload:
            logger.info("Uploading scan results of %d match%s to %s",
                        self.matches, 'es' if self.matches > 1 else '', self.results_url)

        self.host_scan_mutation = self.create_host_scan_mutation()
        logger.debug(self.host_scan_mutation)
        if self.no_upload:
            logger.info('Skipping scan results upload because of --no-upload option')
            return

        response = self.session.post(self.results_url, json={'query': self.host_scan_mutation})
        if response.status_code != 200:
            logger.error("%s: %s", response.status_code, response.text)
            sys.exit(1)
        else:
            logger.info("Scan results uploaded successfully.")

    def create_host_scan_mutation(self):
        # Check if insightsId is a GraphQL field
        # If not, use the hostname instead
        # TODO: find a better way to do this
        insights_id_query = """
        query {
            hosts(first: 1) {
                nodes {
                    insightsId
                }
            }
        }"""
        response = self.session.post(self.results_url, json={'query': insights_id_query})
        if response.status_code == 400 or 'Cannot query field \"insightsId\"' in response.text:
            scanned_host = 'hostname: "%s"' % os.uname()[1]
        else:
            scanned_host = 'insightsId: "%s"' % get_insights_id()

        # Build the mutation text
        mutation_header = """
        mutation {
          recordHostScan(
            input: {
              scannedhost: {
                %s
                rulesScanned: [""" % scanned_host

        mutation_footer = """
                ]
              }
            }
          ) {
            success
          }
        }"""

        mutation = mutation_header
        for rule_name in self.host_scan.keys():
            rule_scan = """{
                ruleName: "%s"
                stringsMatched: [""" % rule_name
            for match in self.host_scan[rule_name]:
                rule_scan += """{
                    source: "%s"
                    stringData: %s
                    stringIdentifier: %s
                    stringOffset: "%s"
                    metadata: "%s"
                }, """ % (match['source'],
                          json.dumps(match['string_data']),
                          json.dumps(match['string_identifier']),
                          match['string_offset'],
                          json.dumps(match['metadata']).replace('"', '\\"'))
            rule_scan += "]}, "
            mutation += rule_scan

        mutation += mutation_footer
        return mutation

    def run(self):
        if os.geteuid() != 0:
            logger.warning("Run this script via sudo to completely scan your system")
            # sys.exit(1)

        self.scan_filesystem()
        self.scan_processes()
        if self.do_filesystem_scan or self.do_process_scan:
            self.upload_host_scan()
        else:
            logger.error("No scans performed, nothing to upload")
            sys.exit(1)

    def _get_config_option(self, section, option, default_value):
        """
        Get the value of a configuration option or, if it doesn't exist, the default_value
        """
        try:
            return self.cfg.get(section, option)
        except Exception:
            logger.debug("Using default value of %s for config item [%s][%s]" % (str(default_value), section, option))
            return default_value
