#!/usr/libexec/platform-python
import json
import os
import sys
import time
import requests
import subprocess
import logging
from iniparse import RawConfigParser
import re
import urllib3

MIN_YARA_VERSION = "4.1.0"
CONF_DIR = "/etc/malware-detection-client/"
CONF_FILE = CONF_DIR + "malware-detection-client.conf"
CONF_FILE_TEMPLATE = """# Where values are provided for the options, these are the defaults

[client]
# Client log level.  Valid options are DEBUG, INFO, WARNING, ERROR, CRITICAL
#log_level=INFO

[yara]
# Specific location of the yara binary file.  Autodetected if not specified.  Example: /usr/local/bin/yara
#location=

# Abort a particular scan if it takes longer than scan_timeout seconds
#scan_timeout=1800

# Nice value to run yara as
#nice=19

# Number of CPU threads yara will use
#cpu_thread_limit=2

[filesystem]
# Scan the files on this system?
#scan_filesystem=True

[processes]
# Scan the processes on this system?
#scan_processes=True

[rules]
# Location of the rules.  Can be any of:
# - the name of a file on the local filesystem containing rules
# - a URL for an endpoint to download a list of rules via GraphQL
# - a URL to download a file containing rules
#location=/tmp/malware_detection_rules.yar
#location=http://127.0.0.1:3000/api/malware-detection/v1/graphql
#location=http://127.0.0.1:3000/api/malware-detection/v1/signatures.yar
location=%s

# Comma separated list of rules to exclude from the scan, eg:
#exclude=SpammyRule, Irrelevant, just_because
#exclude=

[results]
# A GraphQL endpoint for sending the results of a scan
#location=http://127.0.0.1:3000/api/malware-detection/v1/graphql
location=%s

# Limit the number of string matches uploaded per rule, esp if there are a lot of matches for a file/PID
#string_match_limit=10

[auth]
# Authentication options if using an endpoint for rules & results that requires a Keycloak JWT token to access
#keycloak_url=
#username=
#password=
#client_id=
"""


class MalwareDetectionClient(object):

    def __init__(self,
                 rules_location='',
                 results_url='',
                 scan_entity=None,
                 saved_rules_file=None,
                 exclude_rules='',
                 conf_file=CONF_FILE,
                 new_conf_file=None,
                 no_upload=False,
                 debug=None):

        self.cookie = {}
        self.verify = False
        self.matches = 0
        self.no_upload = no_upload

        logging.basicConfig(format="%(asctime)s:%(levelname)s:%(name)s:%(message)s")
        self.logger = logging.getLogger('malware_detection_client')
        self.logger.setLevel('INFO')  # Set a default log level

        self.conf_file = conf_file
        if new_conf_file:
            self._create_new_conf_file(new_conf_file, rules_location, results_url)

        self.cfg = RawConfigParser()
        if self.conf_file:
            if os.path.isfile(self.conf_file):
                self.cfg.read(self.conf_file)
            else:
                self.logger.warning("Couldn't find specified config file '%s', using default values", self.conf_file)

        if debug:
            log_level = 'DEBUG'
        elif os.getenv('LOG_LEVEL'):
            log_level = os.getenv('LOG_LEVEL').upper()
        else:
            log_level = logging.getLevelName(self._get_ini_option('client', 'log_level', 'INFO').upper())
        self.logger.setLevel(log_level)
        logging.getLogger('requests').setLevel(log_level)

        # Find the yara binary on the local system
        self.yara_binary = self._get_ini_option('yara', 'location', None)
        if not self.yara_binary:
            try:
                self.yara_binary = subprocess.check_output("which yara 2>/dev/null", shell=True).strip().decode("utf-8")
            except subprocess.CalledProcessError:
                self.yara_binary = "/usr/local/bin/yara"
        if not os.path.isfile(self.yara_binary):
            self.logger.error("Couldn't find the yara executable, exiting")
            sys.exit(1)
        self.logger.debug("Using yara binary: %s", self.yara_binary)

        # Check the installed yara version >= MIN_YARA_VERSION
        installed_yara_version = subprocess.check_output("yara --version", shell=True).strip()
        try:
            if float(installed_yara_version[:3]) < float(MIN_YARA_VERSION[:3]):
                self.logger.error("Installed yara version %s needs to be >= %s" % (installed_yara_version, MIN_YARA_VERSION))
                sys.exit(1)
        except (IndexError, ValueError):
            # Either the indexing or converting to float went wrong.  Keep going anyway
            pass

        # Check if the -N / symlink option is available
        self.symlink_option = '-N'
        cmd = "%s --help | grep -- '%s'> /dev/null" % (self.yara_binary, self.symlink_option)
        try:
            subprocess.check_output(cmd, shell=True)
            self.logger.debug("Using the --no-follow-symlinks option: '%s'", self.symlink_option)
        except subprocess.CalledProcessError:
            self.symlink_option = ''

        # Get thing to scan from passed in args, if any
        if scan_entity:
            if os.path.exists(scan_entity):
                if re.match('^/[/.]*$', scan_entity):
                    # root directory '/' was specified (not recommended), so expand it to a list of subdirectories
                    self.scan_fsobjects = list(map(lambda x: "/" + x, os.listdir('/')))
                else:
                    self.scan_fsobjects = [scan_entity]
                self.do_filesystem_scan = True
                self.do_process_scan = False
                self.logger.info("Scan argument: %s %s", "directory" if os.path.isdir(scan_entity) else "file", scan_entity)
            elif os.path.exists('/proc/' + scan_entity):
                self.scan_pids = [int(scan_entity)]
                self.do_filesystem_scan = False
                self.do_process_scan = True
                self.logger.info("Scan argument: pid %s", scan_entity)
            else:
                self.logger.error("Couldn't find the specified file/directory/process id to scan: %s.  Exiting", scan_entity)
                sys.exit(1)

        self.nice = int(self._get_ini_option('yara', 'nice', 19))

        # Get the location of the rules, ie a file or URL, or exit if not specified
        self.rules_location = rules_location if rules_location else self._get_ini_option('rules', 'location', '')
        if not self.rules_location:
            self.logger.error("No location specified from which to retrieve the rules")
            sys.exit(1)
        self.logger.info("Getting rules from %s", self.rules_location)

        # Get the location where the scan results are to be uploaded, or exit if not specified
        self.results_url = results_url if results_url else self._get_ini_option('results', 'location', '')
        if self.no_upload:
            self.logger.debug("Scan results will not be uploaded because of --no-upload option")
        else:
            if not self.results_url:
                self.logger.error("No location specified to which to upload results")
                sys.exit(1)
            self.logger.debug("Scan results will be sent to %s", self.results_url)

        # Get a JWT token if keycloak_url is set and we are either downloading rules
        # uploading results via https and we are uploading the results
        if self._get_ini_option('auth', 'keycloak_url', None):
            if self.rules_location.startswith('https') or (self.results_url.startswith('https') and not self.no_upload):
                self._get_jwt_token()

        # Retrieve the rules from the specified URL or file
        if self.rules_location.startswith('http'):
            self.rules_file = saved_rules_file
            self.download_rules()
        else:
            # assume a file on the local system
            self.rules_file = self.rules_location
            if not os.path.isfile(self.rules_file):
                self.logger.error("Couldn't find rules file '%s', exiting", self.rules_file)
                sys.exit(1)

        self.logger.debug("Using rules file %s", self.rules_file)

        # Get the rules to exclude, if any, as a comma separated string and convert to a list
        self.exclude_rules = exclude_rules if exclude_rules else self._get_ini_option('rules', 'exclude', '')
        self.exclude_rules = self.exclude_rules.replace(',', ' ').split()
        if self.exclude_rules:
            self.logger.debug("Rules to exclude: %s", ', '.join(self.exclude_rules))

        # Detect if the rules file is a text or binary (compiled) file (or otherwise)
        cmd = "file -b %s" % self.rules_file
        rule_type = subprocess.check_output(cmd, shell=True).decode('utf-8').strip().lower()
        if os.path.getsize(self.rules_file) == 0 or rule_type == 'empty':
            self.logger.error("Rules file is empty, exiting")
            sys.exit(1)
        self.compiled_rules_flag = '-C' if rule_type.startswith('yara') or rule_type == 'data' else ''
        self.logger.debug("Rules file type: '%s', Compiled rules: %s", rule_type, self.compiled_rules_flag == '-C')

        # Quickly test the rules file to make sure it contains usable rules!
        cmd = "nice -n %d %s --fail-on-warnings -p 1 -f %s %s /dev/null" % \
              (self.nice, self.yara_binary, self.compiled_rules_flag, self.rules_file)
        try:
            subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT)
        except subprocess.CalledProcessError as err:
            error = err.output.strip().decode('utf-8')
            self.logger.error("Unable to use rule file '%s': %s", self.rules_file, error)
            sys.exit(1)

        # Limit the number of threads to limit the CPU load of the scans
        # If system has 2 or fewer CPUs, then use just one thread
        self.cpu_thread_limit = self._get_ini_option('yara', 'cpu_thread_limit', 2)
        nproc = int(subprocess.check_output("nproc", shell=True).strip().decode('utf-8'))
        if nproc <= 2:
            self.cpu_thread_limit = 1
        self.logger.debug("CPU has %d threads, using %d threads for scanning", nproc, self.cpu_thread_limit)

        self.scan_timeout = int(self._get_ini_option('yara', 'scan_timeout', 1800))
        self.string_match_limit = int(self._get_ini_option('results', 'string_match_limit', 10))

        # Dictionary in which to store all the scan matches.  Structure is like ...
        # host_scan = {rule_name: [{source: ..., stringData: ..., stringIdentifier: ..., stringOffset: ...},
        #                          {source: ...}],
        #              rule_name: [{...}, {...}, {...}],
        #              rule_name: [{...}, {...}, {...}],
        #              ... }
        self.host_scan = {}
        self.host_scan_mutation = ''

        if not hasattr(self, 'do_filesystem_scan'):
            self.do_filesystem_scan = bool(self._get_ini_option('filesystem', 'scan_filesystem', True))
        if not hasattr(self, 'do_process_scan'):
            self.do_process_scan = bool(self._get_ini_option('processes', 'scan_processes', True))

    def _get_jwt_token(self):
        """
        Rules &/or results endpoints may require a Keycloak JWT token to access
        The specific auth values are retrieved from in the config file
        Then the JWT token is passed to the backend with every request
        """
        keycloak_url = self._get_ini_option("auth", "keycloak_url", None)
        if keycloak_url:
            self.cookie = self._load_jwt_token()
            if self.cookie:
                return

            self.logger.info("Retrieving JWT token from %s", keycloak_url)
            username = self._get_ini_option("auth", "username", None)
            password = self._get_ini_option("auth", "password", None)
            client_id = self._get_ini_option("auth", "client_id", None)
            auth_data = {'client_id': client_id, 'grant_type': 'password', 'scope': 'openid', 'username': username, 'password': password}
            missing_values = sorted([k for k, v in auth_data.items() if auth_data[k] is None], reverse=True)
            if any(missing_values):
                self.logger.error("Missing authentication value for %s", ', '.join(missing_values))
                sys.exit(1)
            response = requests.post(keycloak_url, data=auth_data)
            if response.status_code != 200:
                self.logger.error("%s: %s", response.status_code, response.text)
                sys.exit(1)
            data = response.json()
            self.cookie = {'cs_jwt': data['id_token'], 'cs_jwt_refresh': data['refresh_token']}
            self._save_jwt_token(self.cookie)

    def download_rules(self):
        """
        Rules can be downloaded a couple of ways:
        - From a GraphQL supplying the rule body in rawRule
        - A file containing either compiled or plain text rule(s)
        """

        from tempfile import NamedTemporaryFile
        import shutil
        urllib3.disable_warnings()

        # File into which the rules are saved
        self.temp_rules_file = NamedTemporaryFile(mode='wb')

        if self.rules_location.endswith('.yar'):
            # Assume a rules file and download it
            response = requests.get(self.rules_location, verify=self.verify, cookies=self.cookie)
            if response.status_code != 200:
                self.logger.error("%s: %s", response.status_code, response.text)
                sys.exit(1)
            self.temp_rules_file.write(response.content)
        else:
            # get rules from GraphQL backend, ignoring any disabled ones
            rules_query = """
            query {
                rules(condition: {isDisabled: false}) {
                    nodes {
                        id
                        name
                        rawRule
                    }
                }
            }"""
            response = requests.post(self.rules_location, json={'query': rules_query},
                                     verify=self.verify, cookies=self.cookie)
            if response.status_code != 200:
                self.logger.error("%s: %s", response.status_code, response.text)
                sys.exit(1)

            data = response.json()
            self.logger.debug(json.dumps(data, indent=2))

            # Do a quick test run of each rule
            # Ignore any that cause errors or warnings or are explicitly excluded
            rule_testing_file = NamedTemporaryFile(mode='wb')
            good_rules, problem_rules = {}, {}
            for rule in data['data']['rules']['nodes']:
                if any([exclude_rule.lower() in rule['name'].lower() for exclude_rule in self.exclude_rules]):
                    problem_rules[rule['name']] = rule['id']
                    continue
                if not rule['rawRule']:
                    problem_rules[rule['name']] = rule['id']
                    continue

                # Write rule to a temporary file and run it to make sure it runs without error
                rule_testing_file.seek(0)
                rule_testing_file.write(rule['rawRule'].encode('utf-8'))
                rule_testing_file.truncate()
                rule_testing_file.flush()
                cmd = "nice -n %d %s --fail-on-warnings -p 1 -f %s /dev/null" % \
                      (self.nice, self.yara_binary, rule_testing_file.name)
                try:
                    subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT)
                    self.logger.debug("Rule '%s' ok, adding", rule['name'])
                    good_rules[rule['name']] = rule['id']
                    self.temp_rules_file.write(rule['rawRule'].encode('utf-8'))
                except subprocess.CalledProcessError as e:
                    self.logger.warning("Rule '%s' failed: '%s', skipping ...", rule['name'], e.output)
                    problem_rules[rule['name']] = rule['id']
                    continue

            self.logger.debug("Rules to use ...")
            self.logger.debug(good_rules)
            self.logger.debug("Rules that won't be used ...")
            self.logger.debug(problem_rules)

        self.temp_rules_file.flush()
        # if -S/--save option used, copy the temp file to the user specified file and use that file
        if self.rules_file:
            self.logger.info("Saving rules to %s", self.rules_file)
            shutil.copy(self.temp_rules_file.name, self.rules_file)
            self.temp_rules_file.close()
        else:
            self.rules_file = self.temp_rules_file.name

    def parse_scan_output(self, output, exclude_items=[]):
        if not output:
            return

        output_lines = output.split("\n")

        # Each 'set' of output lines consists of 1 line containing the rule and file/pid (aka source) it matches
        # Followed by one or more related lines of matching string data from that source, eg
        # ...
        # rule_name source                            + Set of 3 related lines
        # 0x_offset:string_identifier:string_data     |
        # 0x_offset:string_identifier:string_data     +
        # rule_name source                            + Set of 2 related lines
        # 0x_offset:string_identifier:string_data     +
        # ...

        while output_lines:
            if 'error scanning ' in output_lines[0]:
                if output_lines[0].endswith('error: 4'):
                    # ERROR_COULD_NOT_MAP_FILE - only display this yara error if debugging (spammy)
                    self.logger.debug(output_lines[0])
                else:
                    self.logger.info(output_lines[0])
                output_lines.pop(0)  # Skip the error scanning line
                # Skip any string match lines after the error scanning line
                while output_lines and output_lines[0].startswith('0x'):
                    output_lines.pop(0)
                continue
            # Get the rule_name and source from the first line in the set
            try:
                rule_name, source = output_lines[0].strip().split(" ", 1)
            except ValueError as ve:
                # Hopefully shouldn't happen but log it and continue processing
                self.logger.error("Encountered '%s' processing line '%s'" % (ve, output_lines[0]))
                output_lines.pop(0)  # Skip the erroneous line
                # Skip any string match lines afterwards until we get to the next rule match line
                while output_lines and output_lines[0].startswith('0x'):
                    output_lines.pop(0)
                continue

            rule_match = {'rule_name': rule_name, 'matches': []}
            output_lines.pop(0)

            # For the remaining lines in the set, parse the string match data
            string_matches = 0
            while output_lines and output_lines[0].startswith('0x'):
                if string_matches < self.string_match_limit:
                    string_offset, string_identifier, string_data = output_lines[0].split(':', 2)
                    rule_match['matches'].extend([{'source': source,
                                                   'string_data': string_data.strip(),
                                                   'string_identifier': string_identifier,
                                                   'string_offset': int(string_offset, 0)}])
                output_lines.pop(0)
                string_matches += 1

            # If string_match_limit is 0 or there was no string data, still record the file/pid source
            if not rule_match['matches']:
                rule_match['matches'] = [{'source': source,
                                          'string_data': '',
                                          'string_identifier': '',
                                          'string_offset': -1}]

            # ignore this scan match if the source file/pid is to be excluded
            if any([ei in source for ei in exclude_items]):
                continue

            # ignore this scan match if the rule is being excluded
            if any([exclude_rule.lower() in rule_name.lower() for exclude_rule in self.exclude_rules]):
                continue

            self.matches += 1
            self.logger.info("Matched rule %s in %s %s", rule_name, "PID" if source.isdigit() else "file", source)
            self.logger.debug(rule_match)
            if self.host_scan.get(rule_match['rule_name']):
                self.host_scan[rule_match['rule_name']].extend(rule_match['matches'])
            else:
                self.host_scan[rule_match['rule_name']] = rule_match['matches']

    def get_fsobject_list(self, fsobject_list_file, defaults=[]):
        # Parse a file containing a list of filesystem objects to scan, one entry per line
        # Making sure the entries are all valid filesystem objects (files or directories)
        if not os.path.isfile(fsobject_list_file):
            self.logger.warning("Couldn't find '%s', using default values ...", fsobject_list_file)
            return defaults

        fsobject_list = []
        no_objects = True
        for line in open(fsobject_list_file):
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            # Get the first entry on the line and check it exists before adding it
            # If '/', then substitute a list of all the top level directories
            no_objects = False
            obj = line.split()[0]
            if os.path.exists(obj):
                if re.match('^/[/.]*$', obj):
                    # root directory '/' was specified, so expand it to a list of subdirectories
                    fsobject_list.extend(list(map(lambda x: "/" + x, os.listdir('/'))))
                else:
                    fsobject_list.append(obj)

        if no_objects:
            self.logger.warning("No filesystem objects specified in '%s', using default values ...", fsobject_list_file)
            return defaults

        # Remove any duplicates before returning
        return list(set(fsobject_list))

    def scan_filesystem(self):
        if not self.do_filesystem_scan:
            return False

        # Default directories to scan (and not scan) if none are specified by user (via cmdline or config file)
        default_fs_include = ['/bin', '/boot', '/etc', '/home', '/lib', '/lib64',
                              '/opt', '/root', '/tmp', '/usr', '/var']
        default_fs_exclude = ['/cgroup', '/dev', '/media', '/mnt', '/proc', '/run', '/selinux', '/sys']

        if not hasattr(self, 'scan_fsobjects') or not self.scan_fsobjects:
            # Get list of filesystem objects (files/directories) to scan
            fs_include = self.get_fsobject_list(CONF_DIR + 'filesystem_include.txt', defaults=default_fs_include)
            fs_exclude = self.get_fsobject_list(CONF_DIR + 'filesystem_exclude.txt', defaults=default_fs_exclude)
            self.scan_fsobjects = sorted(list(set(fs_include) - set(fs_exclude)))
        self.logger.debug("File system objects to scan: %s", ', '.join(self.scan_fsobjects))

        # Exclude the rules_file, unless that's the thing you specifically wanted to scan
        exclude_files = [] if self.rules_file == ''.join(self.scan_fsobjects) else [self.rules_file]

        self.logger.info("Starting filesystem scan ...")
        fs_scan_start = time.time()

        for scan_fsobject in self.scan_fsobjects:
            scan_start = time.time()
            self.logger.info("Scanning %s ...", scan_fsobject)
            cmd = "nice -n %d %s -s %s -a %d -p %d -r %s %s %s" % \
                  (self.nice, self.yara_binary, self.symlink_option, self.scan_timeout, self.cpu_thread_limit,
                   self.compiled_rules_flag, self.rules_file, scan_fsobject)
            scan_output = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT).decode('utf-8').strip()

            self.parse_scan_output(scan_output, exclude_items=exclude_files)

            scan_end = time.time()
            self.logger.info("Scan time for %s: %d seconds", scan_fsobject, (scan_end - scan_start))

        fs_scan_end = time.time()
        self.logger.info("Filesystem scan time: %s", time.strftime("%H:%M:%S", time.gmtime(fs_scan_end - fs_scan_start)))
        return True

    def scan_processes(self):
        if not self.do_process_scan:
            return False

        self.logger.info("Starting processes scan ...")
        pids_scan_start = time.time()

        if not hasattr(self, 'scan_pids') or not self.scan_pids:
            # Get list of process ids to scan
            all_pids = [entry for entry in os.listdir('/proc') if entry.isdigit()]
            exclude_pids = [str(os.getpid())]  # exclude our script's pid at least
            self.scan_pids = sorted(list(set(all_pids) - set(exclude_pids)), reverse=True)

        for scan_pid in self.scan_pids:
            scan_start = time.time()
            self.logger.info("Scanning pid %s ...", scan_pid)
            cmd = "nice -n %d %s -s %s -a %d -p %d -r %s %s %s" % \
                (self.nice, self.yara_binary, self.symlink_option, self.scan_timeout, self.cpu_thread_limit,
                 self.compiled_rules_flag, self.rules_file, scan_pid)
            try:
                scan_output = subprocess.check_output(cmd, shell=True).decode('utf-8').strip()
            except subprocess.CalledProcessError:
                # Usually if the user doesn't have permissions to scan the process
                continue

            self.parse_scan_output(scan_output)

            scan_end = time.time()
            self.logger.info("Scan time for pid %s: %d seconds", scan_pid, (scan_end - scan_start))

        pids_scan_end = time.time()
        self.logger.info("Processes scan time: %s", time.strftime("%H:%M:%S", time.gmtime(pids_scan_end - pids_scan_start)))
        return True

    def upload_host_scan(self):
        urllib3.disable_warnings()

        host_scan_mutation_header = """
        mutation {
          recordHostScan(
            input: {
              scannedhost: {
                hostname: "%s"
                rulesScanned: [""" % os.uname()[1]

        host_scan_mutation_footer = """
                ]
              }
            }
          ) {
            success
          }
        }"""

        if self.matches == 0:
            self.logger.info("No matches found")
        elif not self.no_upload:
            self.logger.info("Uploading scan results of %d match%s to %s",
                             self.matches, 'es' if self.matches > 1 else '',
                             self.results_url)

        self.host_scan_mutation = host_scan_mutation_header
        for rule_name in self.host_scan.keys():
            rule_scan = """{
                ruleName: "%s"
                stringsMatched: [""" % rule_name
            for match in self.host_scan[rule_name]:
                rule_scan += """{
                    source: "%s"
                    stringData: %s
                    stringIdentifier: %s
                    stringOffset: "%s"
                }, """ % (match['source'], json.dumps(match['string_data']), json.dumps(match['string_identifier']),
                          match['string_offset'])
            rule_scan += "]}, "
            self.host_scan_mutation += rule_scan

        self.host_scan_mutation += host_scan_mutation_footer
        self.logger.debug(self.host_scan_mutation)
        if self.no_upload:
            self.logger.info('Skipping scan results upload because of --no-upload option')
            return

        response = requests.post(self.results_url, json={'query': self.host_scan_mutation},
                                 verify=self.verify, cookies=self.cookie)
        if response.status_code != 200:
            self.logger.error("%s: %s", response.status_code, response.text)
            sys.exit(1)
        else:
            self.logger.info("Scan results uploaded successfully.")

    def run(self):
        if os.geteuid() != 0:
            self.logger.warning("Run this script via sudo to increase privileges to properly scan your system")
            # sys.exit(1)

        if self.scan_filesystem() or self.scan_processes():
            self.upload_host_scan()
        else:
            self.logger.error("No scans performed, nothing to upload")
            sys.exit(1)

    def _create_new_conf_file(self, new_conf_file, rules_location='', results_url=''):
        from six.moves import input

        conf_file_template = CONF_FILE_TEMPLATE % (rules_location, results_url)
        if os.path.isfile(new_conf_file):
            ans = input("Configuration file '%s' already exists.  Overwrite? (y/N) " % new_conf_file)
            if ans.lower() not in ('y', 'yes'):
                self.logger.info("Config file not overwritten.")
                sys.exit(1)

        try:
            with open(new_conf_file, mode='w') as cf:
                cf.write(conf_file_template)
        except Exception as e:
            self.logger.error("Error writing config file: %s", e)
            sys.exit(1)

        self.logger.info("Created new configuration file in %s", new_conf_file)
        sys.exit(0)

    def _get_ini_option(self, section, option, default_value):
        """
        Get the value of a configuration option or, if it doesn't exist, the default_value
        """
        try:
            return self.cfg.get(section, option)
        except Exception:
            self.logger.debug("Using default value of %s for config item [%s][%s]"
                              % (str(default_value), section, option))
            return default_value

    @staticmethod
    def get_jwt_token_cache():
        """
        Get the filename where the JWT token is stored
        """
        from pwd import getpwuid
        user_home_dir = os.getenv('HOME') if os.getenv('HOME') else getpwuid(os.getuid()).pw_dir
        if not user_home_dir or user_home_dir == '/':
            user_home_dir = '/tmp'
        user_conf_dir = os.path.join(user_home_dir, '.malware-detection-client')
        if not os.path.exists(user_conf_dir):
            os.makedirs(user_conf_dir, 0o700)
        return os.path.join(user_conf_dir, 'jwt_token_cache')

    def _load_jwt_token(self):
        """
        Load the JWT token from the user's jwt_token_cache, if the file exists
        and if the token hasn't expired
        """
        import jwt
        from datetime import datetime

        jwt_token_cache = self.get_jwt_token_cache()
        try:
            token = json.loads(open(jwt_token_cache).read())
            decoded_token = jwt.decode(token['cs_jwt'], options={'verify_signature': False}, verify=False)
            token_expiry = datetime.utcfromtimestamp(decoded_token['exp'])
            if token_expiry <= datetime.utcnow():
                self.logger.debug("Locally stored JWT token has expired, requesting a new one")
                return None
            self.logger.debug("Loaded JWT token from %s" % jwt_token_cache)
            return token
        except Exception as e:
            self.logger.debug("Unable to load JWT token from %s: %s" % (jwt_token_cache, str(e)))
            return None

    def _save_jwt_token(self, jwt_token):
        """
        Save the JWT token to the user's jwt_token_cache
        """
        jwt_token_cache = self.get_jwt_token_cache()
        try:
            self.logger.debug("Saving JWT token to %s" % jwt_token_cache)
            with open(jwt_token_cache, 'w', 0o600) as cache:
                cache.write(json.dumps(jwt_token))
        except Exception as e:
            self.logger.debug("Unable to write JWT token to %s: %s" % (jwt_token_cache, str(e)))
