import os
import sys
import json
import subprocess
import logging
import requests

logging.basicConfig(format="%(asctime)s:%(levelname)s:%(message)s")
logger = logging.getLogger('malware_detection_client')
logger.setLevel(logging.getLevelName('INFO'))

CONF_DIR = "/etc/malware-detection-client/"
CONF_FILE = CONF_DIR + "malware-detection-client.conf"
INCLUDE_FILE = CONF_DIR + 'filesystem_include.txt'
EXCLUDE_FILE = CONF_DIR + 'filesystem_exclude.txt'
INSIGHTS_ID_FILE = '/etc/insights-client/machine-id'

CONF_FILE_TEMPLATE = """# Where values are provided for the options, these are the defaults

[client]
# Client log level.  Valid options are DEBUG, INFO, WARNING, ERROR, CRITICAL
#log_level=INFO

# Scan the files on this system?
#scan_filesystem=True

# Scan the processes on this system?
#scan_processes=True

# Add extra metadata to each scan match, eg file type, file md5sum, matching line numbers, process name
#metadata=True

# Location of the rules.  Can be any of:
# - the name of a file on the local filesystem containing rules
# - a URL for an endpoint to download a list of rules via GraphQL
# - a URL to download a file containing rules
#rules_location=/tmp/malware_detection_rules.yar
#rules_location=http://127.0.0.1:3000/api/malware-detection/v1/graphql
#rules_location=http://127.0.0.1:3000/api/malware-detection/v1/signatures.yar
rules_location=%s

# A GraphQL endpoint for sending the results of a scan, eg:
#results_location=http://127.0.0.1:3000/api/malware-detection/v1/graphql
results_location=%s

# Limit the number of string matches uploaded per rule, esp if there are a lot of matches for a file/PID
#string_match_limit=10

# Comma separated list of rules to exclude from the scan, eg SpammyRule, IrrelevantRule, just_because_rule
#exclude_rules=

# URL for your proxy
#proxy_url=


[yara]
# Specific location of the yara binary file.  Autodetected if not specified.  Example: /usr/local/bin/yara
#location=

# Abort a particular scan if it takes longer than scan_timeout seconds
#scan_timeout=3600

# Nice value to run yara as
#nice=19

# Number of CPU threads yara will use
#cpu_thread_limit=2


[auth]
# Subscription-manager certificate and key to use for authenticating to console.redhat.com
cert=/etc/pki/consumer/cert.pem
key=/etc/pki/consumer/key.pem
"""


def create_new_conf_file(new_conf_file, rules_location='', results_url=''):
    """
    Writes out a new configuration file including values for rules_location and results_url (if set)
    If new_conf_file already exists, it will be overwritten
    """
    conf_file_template = CONF_FILE_TEMPLATE % (rules_location, results_url)
    try:
        with open(new_conf_file, mode='w') as cf:
            cf.write(conf_file_template)
    except Exception as e:
        logger.error("Error writing config file: %s", e)
        sys.exit(1)

    logger.info("Created new configuration file in %s", new_conf_file)
    sys.exit(0)


def string_to_bool(s):
    return s.lower() in ("yes", "true", "t", "1")


def get_insights_id():
    """
    Get the machine_id from the insights-client machine-id file
    This file will only exist if the machine is registered with Insights
    """
    try:
        with open(INSIGHTS_ID_FILE) as f:
            insights_id = f.read().strip()
            logger.debug("Obtained insights_id value '%s' from '%s'", insights_id, INSIGHTS_ID_FILE)
            return insights_id
    except Exception as e:
        logger.error("Error reading '%s' to obtain the insightsId: %s.  Ensure the machine is registered with Insights",
                     INSIGHTS_ID_FILE, str(e))
        sys.exit(1)


def get_identity_header(account_number=None):
    from base64 import b64encode
    from socket import gethostname

    if account_number:
        identity = b64encode(json.dumps({'identity': {'account_number': account_number,
                                                      'system': {'cn': gethostname()}}
                                         }).encode('utf-8'))
        return {'x-rh-identity': identity}
    else:
        return {}


def create_session():
    # Create the requests session so it has a default timeout of 2 minutes
    # and retries failed connections 5 times
    from urllib3.util import Retry
    default_timeout = 120  # seconds

    class TimeoutHTTPAdapter(requests.adapters.HTTPAdapter):
        def __init__(self, *args, **kwargs):
            self.timeout = default_timeout
            if "timeout" in kwargs:
                self.timeout = kwargs["timeout"]
                del kwargs["timeout"]
            super(TimeoutHTTPAdapter, self).__init__(*args, **kwargs)

        def send(self, request, **kwargs):
            timeout = kwargs.get("timeout")
            if timeout is None:
                kwargs["timeout"] = self.timeout
            return super(TimeoutHTTPAdapter, self).send(request, **kwargs)

    logger.debug("Creating https session ...")
    session = requests.Session()
    adapter = TimeoutHTTPAdapter(max_retries=Retry(total=5, backoff_factor=1))
    session.mount('https://', adapter)
    session.mount('http://', adapter)
    return session


def find_yara():
    """
    Find the yara binary on the local system.  Use /usr/local/bin/yara if it couldn't be found in the search path.
    """
    for yara in ['/bin/yara', '/usr/bin/yara', '/usr/local/bin/yara']:
        if os.path.isfile(yara):
            return yara

    output = run_cmd(['which', 'yara'])
    if not os.path.isfile(output):
        logger.error("Couldn't find yara.  Please ensure it is installed and in the search path")
        sys.exit(1)

    return output


def run_cmd(cmd):
    """
    cmd is a list containing the command and its arguments, eg
        ['/usr/bin/yara', '-s', '-r', '-C', '/tmp/rules.yar', 'thing_to_scan']
    Since cmd isn't executed via a shell, the full path to the command is necessary.
    universal_newlines means the output is treated as text strings, not binary strings
    """
    if not isinstance(cmd, list):
        logger.error("Command '%s' must be specified as a list of arguments", cmd)
        sys.exit(1)
    try:
        stdout, stderr = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                                          universal_newlines=True).communicate()
        if stderr:
            raise Exception(stderr.strip())
        return stdout.strip()
    except Exception as error:
        logger.info("Problem running command '%s': %s", cmd, str(error))
        return ""


def get_scan_since_timestamp(since):
    """
    Return a unix timestamp corresponding to how long ago to scan for files created/modified since this timestamp.
    Valid values of 'since' are integers > 0 meaning the number of days back in time from now,
                             or 'last' meaning get the timestamp of the last scan
    If 'since' is not one of these valid values, then terminate
    """
    import time
    now = time.time()
    try:
        since_int = int(since)
        if since_int >= 1:
            return now - (since_int * 86400)  # 86400 seconds in a day
        else:
            logger.error("Invalid integer value '%s' for scan_recent.  Values must be > 0", since)
            sys.exit(1)
    except ValueError:
        pass

    if since.lower() == 'last':
        # Get the timestamp of the last scan
        # TODO: not implemented yet - just use since one day ago for the time being
        return now - 86400

    logger.error("Unknown value of '%s' for scan_recent.  "
                 "Valid values for scan_recent are: N - to find files created/modified since N days ago, eg 7 for a week ago, "
                 "or: last - to find files created/modified since the last scan" % since)
    sys.exit(1)


def is_recent_mtime(item, timestamp):
    """
    Return True if the given 'item' has a modification time that is newer than 'timestamp'
    Return False otherwise, or if the the 'item' is a link or another non-file type (eg pipes)
    """
    if os.path.exists(item) and not os.path.islink(item) and os.path.isfile(item):
        return os.path.getmtime(item) > timestamp
    return False


def find_modified_in_directory(directory, timestamp, output_file):
    """
    Find files in 'directory' that have been created/modified since 'timestamp'
    and write their names to 'output_file'
    """
    for root, dirs, files in os.walk(directory):
        for afile in files:
            path = os.path.join(root, afile)
            if is_recent_mtime(path, timestamp):
                output_file.write(path + "\n")


def find_modified_include_items(item_list, timestamp, output_file):
    """
    Find files in the given list of items (files/directories) that have been created/modified since 'timestamp'
    and write their names to 'output_file'
    """
    for item in item_list:
        if os.path.isdir(item):
            find_modified_in_directory(item, timestamp, output_file)
        else:
            if is_recent_mtime(item, timestamp):
                output_file.write(item + '\n')
