#!/usr/libexec/platform-python

import os
import sys
import time
import requests
import subprocess
import logging
import argparse
from configparser import RawConfigParser
import re
import urllib3


class MalwareDetectionClient(object):

    def __init__(self,
                 rules_location='',
                 results_url='',
                 scan_entity=None,
                 save_file=None,
                 conf_file="client.conf",
                 create_conf=None,
                 debug=None):

        self.cookie = {}
        self.verify = False
        self.matches = 0

        logging.basicConfig(format="%(asctime)s:%(levelname)s:%(name)s:%(message)s")
        self.logger = logging.getLogger('malware_detection_client')
        self.logger.setLevel('INFO')  # Set a default log level

        self.conf_file = conf_file
        if create_conf:
            self.create_conf_file(rules_location, results_url)

        self.cfg = RawConfigParser()
        if self.conf_file:
            if os.path.isfile(self.conf_file):
                self.cfg.read(self.conf_file)
            else:
                self.logger.warning("Couldn't find specified config file '%s', using default values", self.conf_file)

        if debug:
            log_level = 'DEBUG'
        elif os.getenv('LOG_LEVEL'):
            log_level = os.getenv('LOG_LEVEL').upper()
        else:
            log_level = logging.getLevelName(self.cfg.get('client', 'log_level', fallback='INFO').upper())
        self.logger.setLevel(log_level)
        logging.getLogger('requests').setLevel(log_level)

        # Find the yara binary on the local system
        self.yara_binary = self.cfg.get('malware_detection_tool', 'location', fallback=None)
        if not self.yara_binary:
            try:
                self.yara_binary = subprocess.check_output("which yara 2>/dev/null", shell=True).strip().decode("utf-8")
            except subprocess.CalledProcessError:
                self.yara_binary = "/usr/local/bin/yara"
        if not os.path.isfile(self.yara_binary):
            self.logger.error("Couldn't find the yara executable, exiting")
            sys.exit(1)
        self.logger.debug("yara binary: %s", self.yara_binary)

        # Check if the -N / symlink option is available
        self.symlink_option = '-N'
        cmd = "%s --help | grep -- '%s'> /dev/null" % (self.yara_binary, self.symlink_option)
        try:
            subprocess.check_output(cmd, shell=True)
            self.logger.debug("Using follow-symlinks option: '%s'", self.symlink_option)
        except subprocess.CalledProcessError:
            self.symlink_option = ''

        if scan_entity:
            # Get thing to scan from passed in args
            if os.path.exists(scan_entity):
                if re.match('^/[/.]*$', scan_entity):
                    # root directory '/' was specified (not recommended), so expand it to a list of subdirectories
                    self.scan_fsobjects = list(map(lambda x: "/" + x, os.listdir('/')))
                else:
                    self.scan_fsobjects = [scan_entity]
                self.do_filesystem_scan = True
                self.do_process_scan = False
                self.logger.info("Scan argument: %s %s", "directory" if os.path.isdir(scan_entity) else "file", scan_entity)
            elif os.path.exists('/proc/' + scan_entity):
                self.scan_pids = [int(scan_entity)]
                self.do_filesystem_scan = False
                self.do_process_scan = True
                self.logger.info("Scan argument: pid %s", scan_entity)
            else:
                self.logger.error("Couldn't find the specified file/directory/process id to scan: %s.  Exiting", scan_entity)
                sys.exit(1)

        self.nice = self.cfg.getint('malware_detection_tool', 'nice', fallback=19)

        # Get the location of the rules, ie a file or URL
        self.rules_location = rules_location if rules_location else self.cfg.get('rules', 'location', fallback='')
        if not self.rules_location:
            self.logger.error("No location specified from which to retrieve the rules, exiting")
            sys.exit(1)
        self.logger.info("Getting rules from %s", self.rules_location)

        self.results_url = results_url if results_url else self.cfg.get('results', 'location', fallback='')
        if not self.results_url:
            self.logger.error("No location specified to which to upload results, exiting")
            sys.exit(1)
        self.logger.debug("Scan results will be sent to %s", self.results_url)

        if self.cfg.get('auth', 'keycloak_url', fallback=None):
            self.get_jwt_token()

        # Retrieve the rules from the specified URL or file
        if self.rules_location.startswith('http'):
            self.rules_file = save_file
            self.download_rules()
        else:
            # assume a file on the local system
            self.rules_file = self.rules_location
            if not os.path.isfile(self.rules_file):
                self.logger.error("Couldn't find rules file '%s', exiting", self.rules_file)
                sys.exit(1)

        self.logger.debug("Using rules file %s", self.rules_file)

        # Detect if the rules file is a text or binary (compiled) file (or otherwise)
        cmd = "file -b %s" % self.rules_file
        rule_type = subprocess.check_output(cmd, shell=True).decode('utf-8').strip().lower()
        if os.path.getsize(self.rules_file) == 0 or rule_type == 'empty':
            self.logger.error("Rules file is empty, exiting")
            sys.exit(1)

        self.compiled_rules_flag = '-C' if rule_type.startswith('yara') or rule_type == 'data' else ''
        self.logger.debug("Rules file type: '%s', Compiled rules: %s", rule_type, self.compiled_rules_flag == '-C')

        # Quickly test the rules file to make sure it contains usable rules!
        cmd = "nice -n %d %s --fail-on-warnings -p 1 -f %s %s /dev/null" % \
              (self.nice, self.yara_binary, self.compiled_rules_flag, self.rules_file)
        try:
            subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT)
        except subprocess.CalledProcessError as err:
            error = err.output.strip().decode('utf-8')
            self.logger.error("Unable to use rule file '%s': %s", self.rules_file, error)
            sys.exit(1)

        # Limit the number of threads to limit the CPU load of the scans
        # If system has 2 or fewer CPUs, then use just one thread
        self.cpu_thread_limit = self.cfg.getint('malware_detection_tool', 'cpu_thread_limit', fallback=2)
        nproc = int(subprocess.check_output("nproc", shell=True).strip().decode('utf-8'))
        if nproc <= 2:
            self.cpu_thread_limit = 1
        self.logger.debug("CPU has %d threads, using %d threads for scanning", nproc, self.cpu_thread_limit)

        self.scan_timeout = self.cfg.getint('malware_detection_tool', 'scan_timeout', fallback=1800)
        self.string_match_limit = self.cfg.getint('results', 'string_match_limit', fallback=10)

        # Dictionary in which to store all the scan matches.  Structure is like ...
        # host_scan = {rule_name: [{source: ..., stringData: ..., stringIdentifier: ..., stringOffset: ...},
        #                          {source: ...}],
        #              rule_name: [{...}, {...}, {...}],
        #              rule_name: [{...}, {...}, {...}],
        #              ... }
        self.host_scan = {}

        if not hasattr(self, 'do_filesystem_scan'):
            self.do_filesystem_scan = self.cfg.getboolean('filesystem', 'scan_filesystem', fallback=True)
        if not hasattr(self, 'do_process_scan'):
            self.do_process_scan = self.cfg.getboolean('processes', 'scan_processes', fallback=True)

    def get_jwt_token(self):
        # Rules &/or results endpoints may require a Keycloak JWT token to access
        # The specific auth values are retrieved from in the config file
        # Then the JWT token is passed to the backend with every request
        keycloak_url = self.cfg.get("auth", "keycloak_url", fallback=None)
        if keycloak_url:
            self.logger.info("Retrieving JWT token from %s", keycloak_url)
            username = self.cfg.get("auth", "username", fallback=None)
            password = self.cfg.get("auth", "password", fallback=None)
            client_id = self.cfg.get("auth", "client_id", fallback=None)
            auth_data = {'client_id': client_id, 'grant_type': 'password', 'scope': 'openid', 'username': username, 'password': password}
            missing_values = [k for k, v in auth_data.items() if auth_data[k] is None]
            if any(missing_values):
                self.logger.error("Missing authentication value for %s", ', '.join(missing_values))
                sys.exit(1)
            response = requests.post(keycloak_url, data=auth_data)
            if response.status_code != 200:
                self.logger.error("%s: %s", response.status_code, response.text)
                sys.exit(1)
            data = response.json()
            self.cookie = {'cs_jwt': data['id_token'], 'cs_jwt_refresh': data['refresh_token']}

    def download_rules(self):
        # Rules can be downloaded a couple of ways:
        # - From a GraphQL supplying the rule body in rawRule
        # - A file containing either compiled or plain text rule(s)

        from tempfile import NamedTemporaryFile
        from json import dumps as json_dumps
        import shutil
        urllib3.disable_warnings()

        # File into which the rules are saved
        self.temp_rules_file = NamedTemporaryFile(mode='wb')

        if self.rules_location.endswith('.yar'):
            # Assume a rules file and download it
            response = requests.get(self.rules_location, verify=self.verify, cookies=self.cookie)
            if response.status_code != 200:
                self.logger.error("%s: %s", response.status_code, response.text)
                sys.exit(1)
            self.temp_rules_file.write(response.content)
        else:
            # get rules from GraphQL backend, ignoring any disabled ones
            rules_query = """
            query {
                rules(condition: {isDisabled: false}) {
                    nodes {
                        id
                        name
                        rawRule
                    }
                }
            }"""
            response = requests.post(self.rules_location, json={'query': rules_query},
                                     verify=self.verify, cookies=self.cookie)
            if response.status_code != 200:
                self.logger.error("%s: %s", response.status_code, response.text)
                sys.exit(1)

            data = response.json()
            self.logger.debug(json_dumps(data, indent=2))

            # Do a quick test run of each rule and ignore any that cause errors or warnings
            rule_testing_file = NamedTemporaryFile(mode='wb')
            exclude_rules = ['url', 'with_sqlite']  # too spammy
            good_rules, problem_rules = {}, {}
            for rule in data['data']['rules']['nodes']:
                if any([er == rule['name'] for er in exclude_rules]):
                    problem_rules[rule['name']] = rule['id']
                    continue
                if not rule['rawRule']:
                    problem_rules[rule['name']] = rule['id']
                    continue

                # Write rule to a temporary file and run it to make sure it runs without error
                rule_testing_file.seek(0)
                rule_testing_file.write(rule['rawRule'].encode('utf-8'))
                rule_testing_file.truncate()
                rule_testing_file.flush()
                cmd = "nice -n %d %s --fail-on-warnings -p 1 -f %s /dev/null" % \
                      (self.nice, self.yara_binary, rule_testing_file.name)
                try:
                    output = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT)
                    self.logger.debug("Rule '%s' ok, adding", rule['name'])
                    good_rules[rule['name']] = rule['id']
                    self.temp_rules_file.write(rule['rawRule'].encode('utf-8'))
                except subprocess.CalledProcessError as e:
                    self.logger.warning("Rule '%s' failed: '%s', skipping ...", rule['name'], e.output)
                    problem_rules[rule['name']] = rule['id']
                    continue

            self.logger.debug("Rules to use ...")
            self.logger.debug(good_rules)
            self.logger.debug("Rules that won't be used ...")
            self.logger.debug(problem_rules)

        self.temp_rules_file.flush()
        # if -S/--save option used, copy the temp file to the user specified file and use that file
        if self.rules_file:
            self.logger.info("Saving rules to %s", self.rules_file)
            shutil.copy(self.temp_rules_file.name, self.rules_file)
            self.temp_rules_file.close()
        else:
            self.rules_file = self.temp_rules_file.name

    def parse_scan_output(self, output, exclude_items=[]):
        if not output:
            return

        output_lines = output.split("\n")

        # Each 'set' of output lines consists of 1 line containing the rule and file/pid (aka source) it matches
        # Followed by one or more related lines of matching string data from that source, eg
        # ...
        # rule_name source                            + Set of 3 related lines
        # 0x_offset:string_identifier:string_data     |
        # 0x_offset:string_identifier:string_data     +
        # rule_name source                            + Set of 2 related lines
        # 0x_offset:string_identifier:string_data     +
        # ...

        while output_lines:
            # Get the rule_name and source from the first line in the set
            try:
                rule_name, source = output_lines[0].strip().split(" ", 1)
            except ValueError:
                # Shouldn't happen but log it anyway and continue processing
                self.logger.error(output_lines[0])
                continue
            rule_match = {'rule_name': rule_name, 'matches': []}
            output_lines.pop(0)

            # For the remaining lines in the set, parse the string match data
            string_matches = 0
            while output_lines and output_lines[0].startswith('0x'):
                if string_matches < self.string_match_limit:
                    string_offset, string_identifier, string_data = output_lines[0].split(':', 2)
                    rule_match['matches'].extend([{'source': source,
                                                   'string_data': string_data,
                                                   'string_identifier': string_identifier,
                                                   'string_offset': int(string_offset, 0)}])
                output_lines.pop(0)
                string_matches += 1

            # If string_match_limit is 0 or there was no string data, still record the file/pid source
            if not rule_match['matches']:
                rule_match['matches'] = [{'source': source,
                                          'string_data': '',
                                          'string_identifier': '',
                                          'string_offset': -1}]

            # ignore this scan match if the source file/pid is to be excluded
            if any([ei in source for ei in exclude_items]):
                continue

            self.matches += 1
            self.logger.info("Matched rule %s in %s %s", rule_name, "file" if os.path.exists(source) else "pid", source)
            self.logger.debug(rule_match)
            if self.host_scan.get(rule_match['rule_name']):
                self.host_scan[rule_match['rule_name']].extend(rule_match['matches'])
            else:
                self.host_scan[rule_match['rule_name']] = rule_match['matches']

    def get_fsobject_list(self, fsobject_list_file, defaults=[]):
        # Parse a file containing a list of filesystem objects to scan, one entry per line
        # Making sure the entries are all valid filesystem objects (files or directories)
        if not os.path.isfile(fsobject_list_file):
            self.logger.warning("Couldn't find '%s', using default values ...", fsobject_list_file)
            return defaults

        fsobject_list = []
        no_objects = True
        for line in open(fsobject_list_file):
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            # Get the first entry on the line and check it exists before adding it
            # If '/', then substitute a list of all the top level directories
            no_objects = False
            obj = line.split()[0]
            if os.path.exists(obj):
                if re.match('^/[/.]*$', obj):
                    # root directory '/' was specified, so expand it to a list of subdirectories
                    fsobject_list.extend(list(map(lambda x: "/" + x, os.listdir('/'))))
                else:
                    fsobject_list.append(obj)

        if no_objects:
            self.logger.warning("No filesystem objects specified in '%s', using default values ...", fsobject_list_file)
            return defaults

        # Remove any duplicates before returning
        return list(set(fsobject_list))

    def scan_filesystem(self):
        if not self.do_filesystem_scan:
            return False

        # Default directories to scan (and not scan) if none are specified by user (via cmdline or config file)
        default_fs_include = ['/bin', '/boot', '/etc', '/home', '/lib', '/lib64',
                              '/opt', '/root', '/tmp', '/usr', '/var']
        default_fs_exclude = ['/cgroup', '/dev', '/media', '/mnt', '/proc', '/run', '/selinux', '/sys']

        if not hasattr(self, 'scan_fsobjects') or not self.scan_fsobjects:
            # Get list of filesystem objects (files/directories) to scan
            fs_include = self.get_fsobject_list('filesystem_include.txt', defaults=default_fs_include)
            fs_exclude = self.get_fsobject_list('filesystem_exclude.txt', defaults=default_fs_exclude)
            self.scan_fsobjects = sorted(list(set(fs_include) - set(fs_exclude)))
        self.logger.debug("File system objects to scan: %s", ', '.join(self.scan_fsobjects))

        # Exclude the rules_file, unless that's the thing you specifically wanted to scan
        exclude_files = [] if self.rules_file == ''.join(self.scan_fsobjects) else [self.rules_file]

        self.logger.info("Starting filesystem scan ...")
        fs_scan_start = time.time()

        for scan_fsobject in self.scan_fsobjects:
            scan_start = time.time()
            self.logger.info("Scanning %s ...", scan_fsobject)
            cmd = "nice -n %d %s -s %s -a %d -p %d -r %s %s %s" % \
                  (self.nice, self.yara_binary, self.symlink_option, self.scan_timeout, self.cpu_thread_limit,
                   self.compiled_rules_flag, self.rules_file, scan_fsobject)
            scan_output = subprocess.check_output(cmd, shell=True).decode('utf-8').strip()

            self.parse_scan_output(scan_output, exclude_items=exclude_files)

            scan_end = time.time()
            self.logger.info("Scan time for %s: %d seconds", scan_fsobject, (scan_end - scan_start))

        fs_scan_end = time.time()
        self.logger.info("Filesystem scan time: %s", time.strftime("%H:%M:%S", time.gmtime(fs_scan_end - fs_scan_start)))
        return True

    def scan_processes(self):
        if not self.do_process_scan:
            return False

        self.logger.info("Starting processes scan ...")
        pids_scan_start = time.time()

        if not hasattr(self, 'scan_pids') or not self.scan_pids:
            # Get list of process ids to scan
            all_pids = [entry for entry in os.listdir('/proc') if entry.isdigit()]
            exclude_pids = [str(os.getpid())]  # exclude our script's pid at least
            self.scan_pids = sorted(list(set(all_pids) - set(exclude_pids)), reverse=True)

        for scan_pid in self.scan_pids:
            scan_start = time.time()
            self.logger.info("Scanning pid %s ...", scan_pid)
            cmd = "nice -n %d %s -s %s -a %d -p %d -r %s %s %s" % \
                (self.nice, self.yara_binary, self.symlink_option, self.scan_timeout, self.cpu_thread_limit,
                 self.compiled_rules_flag, self.rules_file, scan_pid)
            try:
                scan_output = subprocess.check_output(cmd, shell=True).decode('utf-8').strip()
            except subprocess.CalledProcessError:
                # Usually if the user doesn't have permissions to scan the process
                continue

            self.parse_scan_output(scan_output)

            scan_end = time.time()
            self.logger.info("Scan time for pid %s: %d seconds", scan_pid, (scan_end - scan_start))

        pids_scan_end = time.time()
        self.logger.info("Processes scan time: %s", time.strftime("%H:%M:%S", time.gmtime(pids_scan_end - pids_scan_start)))
        return True

    def upload_host_scan_mutation(self):
        from json import dumps
        urllib3.disable_warnings()

        host_scan_mutation_header = """
        mutation {
          recordHostScan(
            input: {
              scannedhost: {
                hostname: "%s"
                rulesScanned: [""" % os.uname()[1]

        host_scan_mutation_footer = """
                ]
              }
            }
          ) {
            success
          }
        }"""

        if self.matches == 0:
            self.logger.info("No matches found")
        else:
            self.logger.info("Uploading scan results of %d match%s to %s",
                             self.matches, 'es' if self.matches > 1 else '',
                             self.results_url)

        host_scan_mutation = host_scan_mutation_header
        for rule_name in self.host_scan.keys():
            rule_scan = """{
                ruleName: "%s"
                stringsMatched: [""" % rule_name
            for match in self.host_scan[rule_name]:
                rule_scan += """{
                    source: "%s"
                    stringData: %s
                    stringIdentifier: %s
                    stringOffset: "%s"
                }, """ % (match['source'], dumps(match['string_data']), dumps(match['string_identifier']),
                          match['string_offset'])
            rule_scan += "]}, "
            host_scan_mutation += rule_scan

        host_scan_mutation += host_scan_mutation_footer
        self.logger.debug(host_scan_mutation)
        response = requests.post(self.results_url, headers={'host_uuid': '00000000-0000-0000-0000-000000000000'},
                                 json={'query': host_scan_mutation}, verify=self.verify, cookies=self.cookie)
        if response.status_code != 200:
            self.logger.error("%s: %s", response.status_code, response.text)
            sys.exit(1)
        else:
            self.logger.info("Scan results uploaded successfully.")

    def run(self):
        if os.geteuid() != 0:
            self.logger.warning("You must be root to properly scan your system")
            # sys.exit(1)

        if self.scan_filesystem() or self.scan_processes():
            self.upload_host_scan_mutation()
        else:
            self.logger.error("No scans performed, nothing to upload")
            sys.exit(1)

    def create_conf_file(self, rules_location='', results_url=''):
        from six.moves import input
        conf_file_template = """# Where values are provided for the options, these are the defaults

[client]
# Client log level.  Valid options are DEBUG, INFO, WARNING, ERROR, CRITICAL
#log_level=INFO

[malware_detection_tool]
# Specific location of the yara binary file.  Autodetected if not specified.  Example: /usr/local/bin/yara
#location=

# Abort a particular scan if it takes longer than scan_timeout seconds
#scan_timeout=1800

# Nice value to run the detection tool as
#nice=19

# Number of CPU threads the detection tool will use
#cpu_thread_limit=2

[filesystem]
# Scan the files on this system?
#scan_filesystem=True

[processes]
# Scan the processes on this system?
#scan_processes=True

[rules]
# Location of the rules.  Can be any of:
# - the name of a file on the local filesystem containing rules
# - a URL for an endpoint to download a list of rules via GraphQL
# - a URL to download a file containing rules
#location=/tmp/malware_detection_rules.yar
#location=http://127.0.0.1:3000/api/malware-detection/v1/graphql
#location=http://127.0.0.1:3000/api/malware-detection/v1/signatures.yar
location=%s

[results]
# A GraphQL endpoint for sending the results of a scan
#location=http://127.0.0.1:3000/api/malware-detection/v1/graphql
location=%s

# Limit the number of string matches uploaded per rule, esp if there are a lot of matches for a file/PID
#string_match_limit=10

[auth]
# Authentication options if using an endpoint for rules & results that requires a Keycloak JWT token to access
#keycloak_url=
#username=
#password=
#client_id=
""" % (rules_location, results_url)
        if os.path.isfile(self.conf_file):
            ans = input("Config file " + self.conf_file + " already exists.  Overwrite? (y/N) ")
            if ans.lower() not in ('y', 'yes'):
                self.logger.info("Config file not overwritten.")
                sys.exit(1)

        try:
            with open(self.conf_file, mode='w') as cf:
                cf.write(conf_file_template)
        except Exception as e:
            self.logger.error("Error writing config file: %s", e)
            sys.exit(1)

        self.logger.info("Created config file in %s", self.conf_file)
        sys.exit(0)


if __name__ == "__main__":
    # Certain values may be supplied as command line arguments to client.py
    p = argparse.ArgumentParser(description="Arguments will override corresponding values from the config files")
    p.add_argument('-r', '--rules', help='Location of the rules, ie HTTP URL or local file', default='',
                   metavar='RULES_LOCATION')
    p.add_argument('-u', '--upload', help='Upload results to the specified URL', default='',
                   metavar='UPLOAD_LOCATION')
    p.add_argument('-s', '--scan', help='Scan only this file, dir or pid', metavar='FILE|DIR|PID')
    p.add_argument('-c', '--conf', help='Specify a configuration file location (default: ./client.conf)',
                   default='client.conf', metavar='CONF_FILE')
    p.add_argument('-S', '--save', help='Save downloaded rules to this file', metavar='RULES_FILE')
    p.add_argument('-C', '--create', action='store_true', help='Create a default configuration file')
    p.add_argument('-d', '--debug', action='store_true', help='Enable debug level logging')
    args = p.parse_args()

    mdc = MalwareDetectionClient(rules_location=args.rules, save_file=args.save, results_url=args.upload,
                                 scan_entity=args.scan, conf_file=args.conf, create_conf=args.create,
                                 debug=args.debug)
    mdc.run()
